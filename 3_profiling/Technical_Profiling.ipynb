{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yF6tXzYdOPUm",
        "u1Zc8PNqOUI5",
        "tzej1hRXB1u4",
        "QkaLRqBNSYlX"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StefanoGiacomelli/e2panns/blob/main/E2PANNs_Model_Profiling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#%cd /content/drive/MyDrive/Stefano_Giacomelli/Tecnojest/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo3PaabKBZAE",
        "outputId": "f5c29ae5-e9ca-4ea5-e408-2f290308db95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1T5OivhNEVmsivuZiIvY5-RrXh2JfbBM2/Tecnojest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install epanns-inference"
      ],
      "metadata": {
        "id": "KeALtfybPKpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f1b18af-f9dc-4f78-f3cc-051f5bb13b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting epanns-inference\n",
            "  Downloading epanns_inference-0.1.1-py3-none-any.whl.metadata (725 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from epanns-inference) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from epanns-inference) (2.6.0+cu124)\n",
            "Collecting torchlibrosa (from epanns-inference)\n",
            "  Downloading torchlibrosa-0.1.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->epanns-inference) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->epanns-inference) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->epanns-inference) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->epanns-inference) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->epanns-inference) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->epanns-inference)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->epanns-inference)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->epanns-inference)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->epanns-inference)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->epanns-inference)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->epanns-inference)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->epanns-inference)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->epanns-inference)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->epanns-inference)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->epanns-inference) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->epanns-inference) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->epanns-inference) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->epanns-inference)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->epanns-inference) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->epanns-inference) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->epanns-inference) (1.3.0)\n",
            "Requirement already satisfied: librosa>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchlibrosa->epanns-inference) (0.11.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.0->torchlibrosa->epanns-inference) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.0->torchlibrosa->epanns-inference) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.0->torchlibrosa->epanns-inference) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.0->torchlibrosa->epanns-inference) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.0->torchlibrosa->epanns-inference) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.0->torchlibrosa->epanns-inference) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.0->torchlibrosa->epanns-inference) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.0->torchlibrosa->epanns-inference) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.0->torchlibrosa->epanns-inference) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.0->torchlibrosa->epanns-inference) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.0->torchlibrosa->epanns-inference) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->epanns-inference) (3.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa>=0.8.0->torchlibrosa->epanns-inference) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa>=0.8.0->torchlibrosa->epanns-inference) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.8.0->torchlibrosa->epanns-inference) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.8.0->torchlibrosa->epanns-inference) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa>=0.8.0->torchlibrosa->epanns-inference) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa>=0.8.0->torchlibrosa->epanns-inference) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.8.0->torchlibrosa->epanns-inference) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->torchlibrosa->epanns-inference) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->torchlibrosa->epanns-inference) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->torchlibrosa->epanns-inference) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.8.0->torchlibrosa->epanns-inference) (2025.1.31)\n",
            "Downloading epanns_inference-0.1.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchlibrosa-0.1.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchlibrosa, epanns-inference\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed epanns-inference-0.1.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchlibrosa-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXlMWuHiA_qy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import subprocess\n",
        "import glob\n",
        "import platform\n",
        "import signal\n",
        "import psutil\n",
        "import tracemalloc\n",
        "from queue import Queue\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import logging\n",
        "import json\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy.stats import iqr, skew, kurtosis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Profiling functions\n",
        "\n",
        "(it replicates the profiling environment on local hardware - w. minor adjustments)"
      ],
      "metadata": {
        "id": "bCx5vALrBKp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "yF6tXzYdOPUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install codecarbon==2.4.2\n",
        "\n",
        "from codecarbon import EmissionsTracker"
      ],
      "metadata": {
        "id": "bIHEeLTJBd56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7eb7191-019a-428e-90c1-a4d06a8ccb40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting codecarbon==2.4.2\n",
            "  Downloading codecarbon-2.4.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting arrow (from codecarbon==2.4.2)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from codecarbon==2.4.2) (8.1.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from codecarbon==2.4.2) (2.2.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from codecarbon==2.4.2) (0.21.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from codecarbon==2.4.2) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from codecarbon==2.4.2) (9.0.0)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from codecarbon==2.4.2) (12.0.0)\n",
            "Collecting rapidfuzz (from codecarbon==2.4.2)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from codecarbon==2.4.2) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from arrow->codecarbon==2.4.2) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon==2.4.2)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon==2.4.2) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon==2.4.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon==2.4.2) (2025.2)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->codecarbon==2.4.2) (12.570.86)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon==2.4.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon==2.4.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon==2.4.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon==2.4.2) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon==2.4.2) (1.17.0)\n",
            "Downloading codecarbon-2.4.2-py3-none-any.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.9/494.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: types-python-dateutil, rapidfuzz, arrow, codecarbon\n",
            "Successfully installed arrow-1.3.0 codecarbon-2.4.2 rapidfuzz-3.13.0 types-python-dateutil-2.9.0.20241206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "\n",
        "# Hardware Profiling ------------------------------------------------------------------------------------------\n",
        "def your_gpu(verbose=True, save_path=None):\n",
        "    \"\"\"\n",
        "    Verify NVIDIA GPU(s) availability, return PyTorch device string and detailed GPU info.\n",
        "    Optionally saves the information into a JSON file if save_path is provided.\n",
        "    \"\"\"\n",
        "\n",
        "    def bytes_to_gb(bytes_val):\n",
        "        return bytes_val * 1e-9\n",
        "\n",
        "    def fetch_gpu_info(gpu_id):\n",
        "        gpu_info = {\"id\": gpu_id}\n",
        "        device = f\"cuda:{gpu_id}\"\n",
        "        gpu_info[\"name\"] = torch.cuda.get_device_name(gpu_id)\n",
        "        try:\n",
        "            free_mem, total_mem = torch.cuda.mem_get_info(device)\n",
        "            gpu_info[\"total_memory_gb\"] = bytes_to_gb(total_mem)\n",
        "            gpu_info[\"free_memory_gb\"] = bytes_to_gb(free_mem)\n",
        "            if verbose:\n",
        "                print(f\"[GPU-{gpu_id}] Name: {gpu_info['name']}, Free: {gpu_info['free_memory_gb']:.2f} GB, Total: {gpu_info['total_memory_gb']:.2f} GB\")\n",
        "        except Exception as e:\n",
        "            gpu_info[\"total_memory_gb\"] = None\n",
        "            gpu_info[\"free_memory_gb\"] = None\n",
        "            if verbose:\n",
        "                print(f\"[GPU-{gpu_id}] Memory info retrieval failed: {e}\")\n",
        "        return gpu_info\n",
        "\n",
        "    gpu_details = {\"gpu_driver_version\": None,\n",
        "                   \"cuda_compiler_version\": None,\n",
        "                   \"count\": 0,\n",
        "                   \"devices\": []}\n",
        "\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            device_string = f\"cuda:{torch.cuda.current_device()}\"\n",
        "            gpu_details[\"count\"] = torch.cuda.device_count()\n",
        "\n",
        "            with ThreadPoolExecutor() as executor:\n",
        "                gpu_details[\"devices\"] = list(executor.map(fetch_gpu_info, range(gpu_details[\"count\"])))\n",
        "\n",
        "            if shutil.which(\"nvidia-smi\"):\n",
        "                try:\n",
        "                    smi_output = subprocess.check_output([\"nvidia-smi\"], encoding=\"utf-8\")\n",
        "                    for line in smi_output.splitlines():\n",
        "                        if \"Driver Version\" in line:\n",
        "                            parts = line.split()\n",
        "                            idx = parts.index(\"Version:\") + 1 if \"Version:\" in parts else None\n",
        "                            if idx and idx < len(parts):\n",
        "                                gpu_details[\"gpu_driver_version\"] = parts[idx]\n",
        "                            break\n",
        "                    if verbose:\n",
        "                        print(f\"Driver Version: {gpu_details['gpu_driver_version']}\")\n",
        "                except Exception as e:\n",
        "                    if verbose:\n",
        "                        print(f\"nvidia-smi output parsing failed: {e}\")\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"'nvidia-smi' not found.\")\n",
        "\n",
        "            if shutil.which(\"nvcc\"):\n",
        "                try:\n",
        "                    nvcc_output = subprocess.check_output([\"nvcc\", \"--version\"], encoding=\"utf-8\")\n",
        "                    for line in nvcc_output.splitlines():\n",
        "                        if \"release\" in line:\n",
        "                            idx = line.index(\"release\") + len(\"release\")\n",
        "                            cuda_version = line[idx:].split(\",\")[0].strip()\n",
        "                            gpu_details[\"cuda_compiler_version\"] = cuda_version\n",
        "                            break\n",
        "                    if verbose:\n",
        "                        print(f\"CUDA Compiler Version: {gpu_details['cuda_compiler_version']}\")\n",
        "                except Exception as e:\n",
        "                    if verbose:\n",
        "                        print(f\"nvcc output parsing failed: {e}\")\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"'nvcc' not found.\")\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "        else:\n",
        "            device_string = \"cpu\"\n",
        "            if verbose:\n",
        "                print(\"No GPU detected. Using CPU.\")\n",
        "                print(f\"PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        device_string = \"cpu\"\n",
        "        if verbose:\n",
        "            print(f\"Error during GPU detection: {e}\")\n",
        "\n",
        "    # Save GPU info to JSON\n",
        "    if save_path is not None:\n",
        "        try:\n",
        "            with open(save_path, \"w\") as f:\n",
        "                json.dump({\"device\": device_string, \"gpu_info\": gpu_details}, f, indent=4)\n",
        "            if verbose:\n",
        "                print(f\"Saved GPU info JSON to: {save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save GPU info JSON: {e}\")\n",
        "\n",
        "    return device_string, gpu_details\n",
        "\n",
        "\n",
        "def your_hardware(verbose=True, save_path=None):\n",
        "    \"\"\"\n",
        "    Inspect and log hardware details (CPU, RAM, Disk) with cross-platform support.\n",
        "    Optionally saves the results into a JSON file if save_path is provided.\n",
        "    \"\"\"\n",
        "    hardware_info = {}\n",
        "\n",
        "    # CPU Info\n",
        "    def get_cpu_info():\n",
        "        if platform.system() == \"Linux\":\n",
        "            try:\n",
        "                output = subprocess.check_output([\"cat\", \"/proc/cpuinfo\"], encoding=\"utf-8\")\n",
        "                if verbose:\n",
        "                    print(\"CPU Info retrieved from /proc/cpuinfo\")\n",
        "                return parse_cpu_info_linux(output)\n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"Failed to retrieve CPU info: {e}\")\n",
        "        return {}\n",
        "\n",
        "    def parse_cpu_info_linux(output):\n",
        "        cpu_model = None\n",
        "        cpu_count = 0\n",
        "        cpuinfo_frequencies = {}\n",
        "\n",
        "        for line in output.splitlines():\n",
        "            if \"model name\" in line:\n",
        "                if cpu_model is None:\n",
        "                    cpu_model = line.split(\":\")[1].strip()\n",
        "                cpu_count += 1\n",
        "\n",
        "        # Try to read per-core frequencies using psutil\n",
        "        try:\n",
        "            freqs = psutil.cpu_freq(percpu=True)\n",
        "            if freqs:\n",
        "                freqs_mhz = {}\n",
        "                for idx, f in enumerate(freqs):\n",
        "                    if f:  # psutil might return None for a core\n",
        "                        freqs_mhz[f\"cpu{idx}\"] = f.max  # Max frequency in MHz\n",
        "                cpuinfo_frequencies = freqs_mhz\n",
        "        except Exception:\n",
        "            cpuinfo_frequencies = {}\n",
        "\n",
        "        return {\"model_name\": cpu_model,\n",
        "                \"physical_cores\": cpu_count,\n",
        "                \"frequencies_mhz\": cpuinfo_frequencies if cpuinfo_frequencies else \"Not Available\"}\n",
        "\n",
        "    hardware_info[\"cpu\"] = get_cpu_info()\n",
        "\n",
        "    # RAM Info\n",
        "    def get_ram_info():\n",
        "        try:\n",
        "            virtual_mem = psutil.virtual_memory()\n",
        "            return {\"total_memory_gb\": round(virtual_mem.total / 1e9, 2),\n",
        "                    \"available_memory_gb\": round(virtual_mem.available / 1e9, 2),\n",
        "                    \"used_memory_gb\": round(virtual_mem.used / 1e9, 2),\n",
        "                    \"percent_used\": virtual_mem.percent}\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to retrieve RAM info: {e}\")\n",
        "            return {}\n",
        "\n",
        "    hardware_info[\"ram\"] = get_ram_info()\n",
        "\n",
        "    # Disk Info\n",
        "    def get_disk_info():\n",
        "        try:\n",
        "            disks = []\n",
        "            if platform.system() in [\"Linux\", \"Darwin\"]:\n",
        "                if shutil.which(\"df\"):\n",
        "                    output = subprocess.check_output([\"df\", \"-h\"], encoding=\"utf-8\")\n",
        "                    lines = output.splitlines()\n",
        "                    headers = lines[0].split()\n",
        "                    for line in lines[1:]:\n",
        "                        if line.strip():\n",
        "                            parts = line.split()\n",
        "                            disk_info = dict(zip(headers, parts))\n",
        "                            disks.append(disk_info)\n",
        "            elif platform.system() == \"Windows\":\n",
        "                for partition in psutil.disk_partitions():\n",
        "                    usage = psutil.disk_usage(partition.mountpoint)\n",
        "                    disks.append({\"device\": partition.device,\n",
        "                                  \"mountpoint\": partition.mountpoint,\n",
        "                                  \"fstype\": partition.fstype,\n",
        "                                  \"total_gb\": round(usage.total / 1e9, 2),\n",
        "                                  \"used_gb\": round(usage.used / 1e9, 2),\n",
        "                                  \"free_gb\": round(usage.free / 1e9, 2),\n",
        "                                  \"percent_used\": usage.percent})\n",
        "            return disks\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to retrieve disk info: {e}\")\n",
        "            return []\n",
        "\n",
        "    hardware_info[\"disks\"] = get_disk_info()\n",
        "\n",
        "    # Optional: Print Hardware summary\n",
        "    if verbose:\n",
        "        print(f\"Hardware Summary: {json.dumps(hardware_info, indent=4)}\")\n",
        "\n",
        "    # Optional: Save Hardware summary to JSON\n",
        "    if save_path is not None:\n",
        "        try:\n",
        "            with open(save_path, \"w\") as f:\n",
        "                json.dump(hardware_info, f, indent=4)\n",
        "            if verbose:\n",
        "                print(f\"Saved hardware info JSON to: {save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save hardware info JSON: {e}\")\n",
        "\n",
        "    return hardware_info\n",
        "\n",
        "\n",
        "# Units Monitoring Functions ----------------------------------------------------------------------------------\n",
        "cpu_usage_samples = Queue()\n",
        "cpu_monitoring = threading.Event()\n",
        "gpu_usage_samples = Queue()\n",
        "gpu_monitoring = threading.Event()\n",
        "\n",
        "\n",
        "def monitor_cpu_usage():\n",
        "    \"\"\"\n",
        "    Continuously monitor CPU resources usage and append utilization samples to a thread-safe queue.\n",
        "\n",
        "    :global cpu_usage_samples: A thread-safe queue to store CPU usage percentages.\n",
        "    :type cpu_usage_samples: Queue\n",
        "    :global cpu_monitoring: A thread-safe event to control the monitoring loop.\n",
        "    :type cpu_monitoring: threading.Event\n",
        "    \"\"\"\n",
        "    if not cpu_monitoring.is_set():\n",
        "        cpu_monitoring.set()\n",
        "\n",
        "    while cpu_monitoring.is_set():\n",
        "        cpu_usage_samples.put(psutil.cpu_percent(interval=0.1))\n",
        "\n",
        "\n",
        "def monitor_gpu_usage():\n",
        "    \"\"\"\n",
        "    Continuously monitor GPU utilization and append samples to a thread-safe queue.\n",
        "\n",
        "    :global gpu_usage_samples: A thread-safe queue to store GPU usage percentages.\n",
        "    :type gpu_usage_samples: Queue\n",
        "    :global gpu_monitoring: A thread-safe event to control the monitoring loop.\n",
        "    :type gpu_monitoring: threading.Event\n",
        "    \"\"\"\n",
        "    if not gpu_monitoring.is_set():\n",
        "        gpu_monitoring.set()\n",
        "\n",
        "    while gpu_monitoring.is_set():\n",
        "        try:\n",
        "            result = subprocess.run([\"nvidia-smi\", \"--query-gpu=utilization.gpu\", \"--format=csv,noheader,nounits\"],\n",
        "                                    stdout=subprocess.PIPE,\n",
        "                                    stderr=subprocess.PIPE,\n",
        "                                    universal_newlines=True)\n",
        "            if result.returncode == 0:\n",
        "                utilization = int(result.stdout.strip())\n",
        "                gpu_usage_samples.put(utilization)\n",
        "        except Exception as e:\n",
        "            gpu_usage_samples.put(0)  # Assume 0% usage if query fails\n",
        "\n",
        "        time.sleep(0.1)\n",
        "\n",
        "\n",
        "# Model profiling functions -----------------------------------------------------------------------------------\n",
        "def min_binary_search(model, sample_rate, device, save_path, verbose=True):\n",
        "    \"\"\"\n",
        "    Find the minimum input duration the model can process without error using binary search.\n",
        "    Save (or append) the result to a specified JSON file.\n",
        "\n",
        "    :param model: Your PyTorch model.\n",
        "    :param sample_rate: Audio sample rate (e.g., 32000).\n",
        "    :param device: Device string ('cpu' or 'cuda').\n",
        "    :param save_path: Filepath to save JSON results.\n",
        "    :param verbose: Whether to print progress info.\n",
        "    \"\"\"\n",
        "    def generate_input(duration_samples, device):\n",
        "        return torch.randn((1, duration_samples), device=device) * 2 - 1.\n",
        "\n",
        "    max_dur = int(sample_rate * 10)  # 10 seconds max\n",
        "    low, high = 1, max_dur\n",
        "    total_iterations = high - low + 1\n",
        "\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        with tqdm(total=total_iterations, desc=\"MIN Input Size Binary Search\") as pbar:\n",
        "            i = 0\n",
        "            while low < high and (high - low) > 1:\n",
        "                mid = (high + low) // 2\n",
        "                try:\n",
        "                    set_seeds(42)\n",
        "                    x = generate_input(mid, device)\n",
        "                    output = model(x.float())\n",
        "                    high = mid - 1\n",
        "                except Exception as e:\n",
        "                    low = mid\n",
        "                i += 1\n",
        "                completed_iterations = total_iterations - (high - low + 1)\n",
        "                pbar.n = completed_iterations\n",
        "                pbar.refresh()\n",
        "\n",
        "    # Final results\n",
        "    min_samples = high\n",
        "    min_seconds = min_samples / sample_rate\n",
        "\n",
        "    results_entry = {\"min_input_size\": {\"samples\": int(min_samples),\n",
        "                                        \"seconds\": float(min_seconds),\n",
        "                                        \"sample_rate\": int(sample_rate),\n",
        "                                        \"binary_search_iterations\": int(i)}}\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Results: {json.dumps(results_entry, indent=4)}\")\n",
        "\n",
        "    # Save results to JSON\n",
        "    if save_path is not None:\n",
        "        try:\n",
        "            if os.path.exists(save_path):\n",
        "                with open(save_path, \"r\") as f:\n",
        "                    existing_data = json.load(f)\n",
        "                if not isinstance(existing_data, dict):\n",
        "                    existing_data = {}\n",
        "            else:\n",
        "                existing_data = {}\n",
        "\n",
        "            # Update\n",
        "            existing_data.update(results_entry)\n",
        "\n",
        "            with open(save_path, \"w\") as f:\n",
        "                json.dump(existing_data, f, indent=4)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Saved (updated) profiling results to: {save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save profiling results: {e}\")\n",
        "\n",
        "    return results_entry\n",
        "\n",
        "\n",
        "def overall_time(model, sample_rate, device, save_path, iterations=100, input_duration_sec=10.0, verbose=True, npz_save_path=None):\n",
        "    \"\"\"\n",
        "    Profile the overall wall clock time for the model inference (sleep included).\n",
        "    Save (or append) the result to a specified JSON file and optionally save NPZ arrays.\n",
        "\n",
        "    :param model: Your PyTorch model.\n",
        "    :param sample_rate: Audio sample rate (e.g., 32000).\n",
        "    :param device: Device string ('cpu' or 'cuda').\n",
        "    :param save_path: Filepath to save JSON results.\n",
        "    :param iterations: Number of iterations to average.\n",
        "    :param input_duration_sec: Duration of the input (seconds). Default 10s.\n",
        "    :param verbose: Whether to print progress info.\n",
        "    :param npz_save_path: Optional path to save .npz compressed timings.\n",
        "    \"\"\"\n",
        "    def generate_input(duration_samples, device):\n",
        "        return torch.randn((1, duration_samples), device=device) * 2 - 1.\n",
        "\n",
        "    samples = int(sample_rate * input_duration_sec)\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    timings = []\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        x = generate_input(samples, device)\n",
        "\n",
        "        for _ in tqdm(range(iterations), desc=\"CPU Overall Time Profiling\"):\n",
        "            set_seeds(42)\n",
        "            start_time = time.perf_counter()\n",
        "            output = model(x.float())\n",
        "            torch.cuda.synchronize() if device.startswith(\"cuda\") else None\n",
        "            elapsed = time.perf_counter() - start_time\n",
        "            timings.append(elapsed)\n",
        "\n",
        "    timings = np.array(timings)\n",
        "\n",
        "    results_entry = {\"cpu_overall_time\": {\"iterations\": int(iterations),\n",
        "                                          \"input_duration_sec\": float(input_duration_sec),\n",
        "                                          \"max_sec\": float(np.max(timings)),\n",
        "                                          \"min_sec\": float(np.min(timings)),\n",
        "                                          \"mean_sec\": float(np.mean(timings)),\n",
        "                                          \"std_dev_sec\": float(np.std(timings, ddof=1)),\n",
        "                                          \"median_sec\": float(np.median(timings)),\n",
        "                                          \"percentiles\": {\"25th_perc\": float(np.percentile(timings, 25)),\n",
        "                                                          \"33th_perc\": float(np.percentile(timings, 33)),\n",
        "                                                          \"66th_perc\": float(np.percentile(timings, 66)),\n",
        "                                                          \"75th_perc\": float(np.percentile(timings, 75))},\n",
        "                                          \"iqr_sec\": float(iqr(timings)),\n",
        "                                          \"skewness\": float(skew(timings)),\n",
        "                                          \"kurtosis\": float(kurtosis(timings))}}\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Results: {json.dumps(results_entry, indent=4)}\")\n",
        "\n",
        "    # Save results to JSON\n",
        "    if save_path is not None:\n",
        "        try:\n",
        "            if os.path.exists(save_path):\n",
        "                with open(save_path, \"r\") as f:\n",
        "                    existing_data = json.load(f)\n",
        "                if not isinstance(existing_data, dict):\n",
        "                    existing_data = {}\n",
        "            else:\n",
        "                existing_data = {}\n",
        "\n",
        "            existing_data.update(results_entry)\n",
        "\n",
        "            with open(save_path, \"w\") as f:\n",
        "                json.dump(existing_data, f, indent=4)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Saved (updated) profiling results to: {save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save profiling results: {e}\")\n",
        "\n",
        "    # Save results to .NPZ\n",
        "    if npz_save_path is not None:\n",
        "        try:\n",
        "            np.savez_compressed(npz_save_path, values=timings, features=np.array(list(results_entry[\"cpu_overall_time\"].items())))\n",
        "            if verbose:\n",
        "                print(f\"Saved compressed NPZ data to: {npz_save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save NPZ: {e}\")\n",
        "\n",
        "    return results_entry\n",
        "\n",
        "\n",
        "def process_time(model, sample_rate, device, save_path, iterations=100, input_duration_sec=10.0, verbose=True, npz_save_path=None):\n",
        "    \"\"\"\n",
        "    Profile the CPU process time (excluding sleep) for model inference.\n",
        "    Save (or append) the result to a specified JSON file and optionally save NPZ arrays.\n",
        "\n",
        "    :param model: Your PyTorch model.\n",
        "    :param sample_rate: Audio sample rate (e.g., 32000).\n",
        "    :param device: Device string ('cpu' or 'cuda').\n",
        "    :param save_path: Filepath to save JSON results.\n",
        "    :param iterations: Number of iterations to average.\n",
        "    :param input_duration_sec: Duration of the input (seconds). Default 10s.\n",
        "    :param verbose: Whether to print progress info.\n",
        "    :param npz_save_path: Optional path to save .npz compressed timings.\n",
        "    \"\"\"\n",
        "    def generate_input(duration_samples, device):\n",
        "        return torch.randn((1, duration_samples), device=device) * 2 - 1.\n",
        "\n",
        "    samples = int(sample_rate * input_duration_sec)\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    timings = []\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        x = generate_input(samples, device)\n",
        "\n",
        "        for _ in tqdm(range(iterations), desc=\"CPU Process Time Profiling\"):\n",
        "            set_seeds(42)\n",
        "            start_time = time.process_time()\n",
        "            output = model(x.float())\n",
        "            torch.cuda.synchronize() if device.startswith(\"cuda\") else None\n",
        "            elapsed = time.process_time() - start_time\n",
        "            timings.append(elapsed)\n",
        "\n",
        "    timings = np.array(timings)\n",
        "\n",
        "    results_entry = {\"cpu_process_time\": {\"iterations\": int(iterations),\n",
        "                                          \"input_duration_sec\": float(input_duration_sec),\n",
        "                                          \"max_sec\": float(np.max(timings)),\n",
        "                                          \"min_sec\": float(np.min(timings)),\n",
        "                                          \"mean_sec\": float(np.mean(timings)),\n",
        "                                          \"std_dev_sec\": float(np.std(timings, ddof=1)),\n",
        "                                          \"median_sec\": float(np.median(timings)),\n",
        "                                          \"percentiles\": {\"25th_perc\": float(np.percentile(timings, 25)),\n",
        "                                                          \"33th_perc\": float(np.percentile(timings, 33)),\n",
        "                                                          \"66th_perc\": float(np.percentile(timings, 66)),\n",
        "                                                          \"75th_perc\": float(np.percentile(timings, 75))},\n",
        "                                          \"iqr_sec\": float(iqr(timings)),\n",
        "                                          \"skewness\": float(skew(timings)),\n",
        "                                          \"kurtosis\": float(kurtosis(timings))}}\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Results: {json.dumps(results_entry, indent=4)}\")\n",
        "\n",
        "    # Save results to JSON\n",
        "    if save_path is not None:\n",
        "        try:\n",
        "            if os.path.exists(save_path):\n",
        "                with open(save_path, \"r\") as f:\n",
        "                    existing_data = json.load(f)\n",
        "                if not isinstance(existing_data, dict):\n",
        "                    existing_data = {}\n",
        "            else:\n",
        "                existing_data = {}\n",
        "\n",
        "            existing_data.update(results_entry)\n",
        "\n",
        "            with open(save_path, \"w\") as f:\n",
        "                json.dump(existing_data, f, indent=4)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Saved (updated) profiling results to: {save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save profiling results: {e}\")\n",
        "\n",
        "    # Save results to .NPZ\n",
        "    if npz_save_path is not None:\n",
        "        try:\n",
        "            np.savez_compressed(npz_save_path, values=timings, features=np.array(list(results_entry[\"cpu_process_time\"].items())))\n",
        "            if verbose:\n",
        "                print(f\"Saved compressed NPZ data to: {npz_save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save NPZ: {e}\")\n",
        "\n",
        "    return results_entry\n",
        "\n",
        "\n",
        "def memory(model, sample_rate, device, save_path=None, iterations=100, input_duration_sec=10.0, verbose=True, npz_save_path=None):\n",
        "    \"\"\"\n",
        "    Profile peak RAM memory during model inference using tracemalloc.\n",
        "    Save (or update) the results into a specified JSON file and optionally into compressed NPZ arrays.\n",
        "    Colab-friendly: no cache/cpu-cycle profiling.\n",
        "    \"\"\"\n",
        "\n",
        "    def generate_input(duration_samples, device):\n",
        "        return torch.randn((1, duration_samples), device=device) * 2 - 1.\n",
        "\n",
        "    samples = int(sample_rate * input_duration_sec)\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        x = generate_input(samples, device)\n",
        "\n",
        "        # Start tracemalloc for memory profiling\n",
        "        tracemalloc.start()\n",
        "\n",
        "        # Model inference\n",
        "        for _ in tqdm(range(iterations), desc=\"Memory Profiling Only\"):\n",
        "            _ = model(x.float())\n",
        "\n",
        "        # Stop tracemalloc\n",
        "        current, peak = tracemalloc.get_traced_memory()\n",
        "        tracemalloc.stop()\n",
        "\n",
        "    results_entry = {\"memory_usage\": {\"iterations\": int(iterations),\n",
        "                                      \"input_duration_sec\": float(input_duration_sec),\n",
        "                                      \"current_bytes\": int(current),\n",
        "                                      \"peak_bytes\": int(peak),\n",
        "                                      \"current_megabytes\": round(current / (1024 ** 2), 4),\n",
        "                                      \"peak_megabytes\": round(peak / (1024 ** 2), 4)}}\n",
        "\n",
        "    if verbose:\n",
        "        print(json.dumps(results_entry, indent=4))\n",
        "\n",
        "    # Save results to JSON\n",
        "    if save_path is not None:\n",
        "        try:\n",
        "            if os.path.exists(save_path):\n",
        "                with open(save_path, \"r\") as f:\n",
        "                    existing_data = json.load(f)\n",
        "                if not isinstance(existing_data, dict):\n",
        "                    existing_data = {}\n",
        "            else:\n",
        "                existing_data = {}\n",
        "\n",
        "            existing_data.update(results_entry)\n",
        "\n",
        "            with open(save_path, \"w\") as f:\n",
        "                json.dump(existing_data, f, indent=4)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Saved (updated) memory profiling results to: {save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save memory profiling results: {e}\")\n",
        "\n",
        "    # Save results to .npz\n",
        "    if npz_save_path is not None:\n",
        "        try:\n",
        "            np.savez_compressed(npz_save_path,\n",
        "                                peak_memory=np.array([current, peak]))\n",
        "            if verbose:\n",
        "                print(f\"Saved compressed NPZ memory data to: {npz_save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save NPZ: {e}\")\n",
        "\n",
        "\n",
        "def cpu_usage(model, sample_rate, device, save_path, iterations=100, input_duration_sec=10.0, verbose=True, npz_save_path=None):\n",
        "    \"\"\"\n",
        "    Profile CPU usage percentage during model inference.\n",
        "    Save (or append) the results into a specified JSON file and optionally into compressed NPZ arrays.\n",
        "\n",
        "    :param model: Your PyTorch model.\n",
        "    :param sample_rate: Audio sample rate (e.g., 32000).\n",
        "    :param device: Device string ('cpu' or 'cuda').\n",
        "    :param save_path: Path to save the JSON results.\n",
        "    :param iterations: Number of iterations to average.\n",
        "    :param input_duration_sec: Duration of the input (seconds). Default 10s.\n",
        "    :param verbose: Whether to print progress info.\n",
        "    :param npz_save_path: Optional path to save .npz compressed results.\n",
        "    \"\"\"\n",
        "    def generate_input(duration_samples, device):\n",
        "        return torch.randn((1, duration_samples), device=device) * 2 - 1.\n",
        "\n",
        "    samples = int(sample_rate * input_duration_sec)\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        x = generate_input(samples, device)\n",
        "\n",
        "        # Start CPU usage monitoring in a separate thread\n",
        "        try:\n",
        "            cpu_monitoring.set()\n",
        "            monitor_thread = threading.Thread(target=monitor_cpu_usage)\n",
        "            monitor_thread.start()\n",
        "\n",
        "            # Run model inference\n",
        "            for _ in tqdm(range(iterations), desc=\"CPU Usage Profiling\"):\n",
        "                _ = model(x.float())\n",
        "\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"[ERROR] During CPU usage monitoring: {e}\")\n",
        "\n",
        "        finally:\n",
        "            cpu_monitoring.clear()\n",
        "            monitor_thread.join()\n",
        "\n",
        "    # Process CPU usage samples\n",
        "    cpu_perc_samples = []\n",
        "    while not cpu_usage_samples.empty():\n",
        "        cpu_perc_samples.append(cpu_usage_samples.get())\n",
        "\n",
        "    avg_cpu_usage = sum(cpu_perc_samples) / len(cpu_perc_samples) if cpu_perc_samples else 0\n",
        "    peak_cpu_usage = max(cpu_perc_samples) if cpu_perc_samples else 0\n",
        "\n",
        "    results_entry = {\"cpu_usage\": {\"iterations\": int(iterations),\n",
        "                                   \"input_duration_sec\": float(input_duration_sec),\n",
        "                                   \"avg_cpu_usage_percent\": round(avg_cpu_usage, 2),\n",
        "                                   \"peak_cpu_usage_percent\": round(peak_cpu_usage, 2)}}\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Results: {json.dumps(results_entry, indent=4)}\")\n",
        "\n",
        "    # Save results to JSON\n",
        "    if save_path is not None:\n",
        "        try:\n",
        "            if os.path.exists(save_path):\n",
        "                with open(save_path, \"r\") as f:\n",
        "                    existing_data = json.load(f)\n",
        "                if not isinstance(existing_data, dict):\n",
        "                    existing_data = {}\n",
        "            else:\n",
        "                existing_data = {}\n",
        "\n",
        "            existing_data.update(results_entry)\n",
        "\n",
        "            with open(save_path, \"w\") as f:\n",
        "                json.dump(existing_data, f, indent=4)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Saved (updated) CPU usage results to: {save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save CPU usage results: {e}\")\n",
        "\n",
        "    # Save results to NPZ (optional)\n",
        "    if npz_save_path is not None:\n",
        "        try:\n",
        "            np.savez_compressed(npz_save_path,\n",
        "                                cpu_usage_samples=np.array(cpu_perc_samples),\n",
        "                                features=np.array(list(results_entry[\"cpu_usage\"].items())))\n",
        "            if verbose:\n",
        "                print(f\"Saved compressed NPZ CPU usage data to: {npz_save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save CPU usage NPZ: {e}\")\n",
        "\n",
        "    return results_entry\n",
        "\n",
        "\n",
        "def energy_co2(model, sample_rate, device, save_path=None, iterations=100, input_duration_sec=10.0, verbose=True, npz_save_path=None):\n",
        "    \"\"\"\n",
        "    Profile energy consumption and CO₂ emissions during model inference using CodeCarbon.\n",
        "    Save results into a specified JSON file (append behavior) and optionally into compressed NPZ arrays.\n",
        "    Runtime printing only. Colab-friendly version.\n",
        "    \"\"\"\n",
        "    def generate_input(duration_samples, device):\n",
        "        return torch.randn((1, duration_samples), device=device) * 2 - 1.\n",
        "\n",
        "    samples = int(sample_rate * input_duration_sec)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    save_dir = os.path.dirname(save_path) if save_path else \"./\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Setup CodeCarbon tracker\n",
        "    energy_tracker = EmissionsTracker(project_name=\"energy_emissions_colab\",\n",
        "                                      tracking_mode=\"machine\",\n",
        "                                      save_to_file=True,\n",
        "                                      output_dir=save_dir,\n",
        "                                      output_file=\"energy_emissions.csv\",\n",
        "                                      measure_power_secs=0.1)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        x = generate_input(samples, device)\n",
        "\n",
        "        energy_tracker.start()\n",
        "\n",
        "        for i in tqdm(range(iterations), desc=\"Energy/CO₂ Emissions Profiling\"):\n",
        "            energy_tracker.start_task(f\"Run-{i+1}\")\n",
        "            _ = model(x.float())\n",
        "            energy_tracker.stop_task(f\"Run-{i+1}\")\n",
        "\n",
        "        energy_tracker.stop()\n",
        "\n",
        "    # Read and process results\n",
        "    emissions_csv_path = os.path.join(save_dir, \"energy_emissions.csv\")\n",
        "    if not os.path.exists(emissions_csv_path):\n",
        "        raise FileNotFoundError(f\"No emissions CSV file found at {emissions_csv_path}.\")\n",
        "\n",
        "    emissions_rate_values = []\n",
        "    cpu_energy_values = []\n",
        "    ram_energy_values = []\n",
        "\n",
        "    with open(emissions_csv_path, 'r') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        for row in reader:\n",
        "            emissions_rate_values.append(float(row.get('emissions_rate', 0)))\n",
        "            cpu_energy_values.append(float(row.get('cpu_energy', 0)))\n",
        "            ram_energy_values.append(float(row.get('ram_energy', 0)))\n",
        "\n",
        "    results_entry = {\"energy_consumption\": {\"iterations\": int(iterations),\n",
        "                                            \"input_duration_sec\": float(input_duration_sec),\n",
        "                                            \"avg_emission_rate_gCO2eq_per_sec\": np.mean(emissions_rate_values) if emissions_rate_values else 0,\n",
        "                                            \"avg_cpu_energy_kWh\": np.mean(cpu_energy_values) if cpu_energy_values else 0,\n",
        "                                            \"avg_ram_energy_kWh\": np.mean(ram_energy_values) if ram_energy_values else 0}}\n",
        "\n",
        "    if verbose:\n",
        "        print(json.dumps(results_entry, indent=4))\n",
        "\n",
        "    # Save results to JSON (append if exists)\n",
        "    if save_path is not None:\n",
        "        try:\n",
        "            if os.path.exists(save_path):\n",
        "                with open(save_path, \"r\") as f:\n",
        "                    existing_data = json.load(f)\n",
        "                if not isinstance(existing_data, dict):\n",
        "                    existing_data = {}\n",
        "            else:\n",
        "                existing_data = {}\n",
        "\n",
        "            # Merge new results\n",
        "            existing_data.update(results_entry)\n",
        "\n",
        "            with open(save_path, \"w\") as f:\n",
        "                json.dump(existing_data, f, indent=4)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Saved (updated) energy profiling results to: {save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save JSON: {e}\")\n",
        "\n",
        "    # Save NPZ (optional)\n",
        "    if npz_save_path is not None:\n",
        "        try:\n",
        "            np.savez_compressed(npz_save_path,\n",
        "                                emissions_rate=np.array(emissions_rate_values),\n",
        "                                cpu_energy=np.array(cpu_energy_values),\n",
        "                                ram_energy=np.array(ram_energy_values))\n",
        "            if verbose:\n",
        "                print(f\"Saved compressed NPZ energy data to: {npz_save_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save NPZ: {e}\")\n",
        "\n",
        "    return results_entry\n",
        "\n",
        "\n",
        "def cuda_time(model, sample_rate, device, save_path, iterations=100, input_duration_sec=10.0, verbose=True, npz_save_path=None):\n",
        "    \"\"\"\n",
        "    Measure CUDA event timing (GPU only) for model inference.\n",
        "\n",
        "    :param model: Your PyTorch model.\n",
        "    :param sample_rate: Audio sample rate (e.g., 32000).\n",
        "    :param device: Device string ('cuda' required).\n",
        "    :param save_path: Path to save the JSON results.\n",
        "    :param iterations: Number of iterations to average.\n",
        "    :param input_duration_sec: Input duration in seconds.\n",
        "    :param verbose: Print verbose output.\n",
        "    :param npz_save_path: Optional path to save .npz results.\n",
        "    \"\"\"\n",
        "    assert device.startswith(\"cuda\"), \"CUDA Event profiling requires a GPU device.\"\n",
        "\n",
        "    def generate_input(duration_samples, device):\n",
        "        return torch.randn((1, duration_samples), device=device) * 2 - 1.\n",
        "\n",
        "    samples = int(sample_rate * input_duration_sec)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    timings = []\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        x = generate_input(samples, device)\n",
        "\n",
        "        for _ in tqdm(range(iterations), desc=\"CUDA Event Timing\"):\n",
        "            set_seeds(42)\n",
        "            start.record()\n",
        "            _ = model(x.float())\n",
        "            end.record()\n",
        "            torch.cuda.synchronize()\n",
        "            timings.append(start.elapsed_time(end) / 1000.)  # Convert ms -> seconds\n",
        "\n",
        "    timings = np.array(timings)\n",
        "\n",
        "    results_entry = {\"cuda_time\": {\"iterations\": int(iterations),\n",
        "                                   \"input_duration_sec\": float(input_duration_sec),\n",
        "                                   \"max_sec\": float(np.max(timings)),\n",
        "                                   \"min_sec\": float(np.min(timings)),\n",
        "                                   \"mean_sec\": float(np.mean(timings)),\n",
        "                                   \"std_dev_sec\": float(np.std(timings, ddof=1)),\n",
        "                                   \"median_sec\": float(np.median(timings)),\n",
        "                                   \"percentiles\": {f\"{p}th_perc\": float(np.percentile(timings, p)) for p in [25, 33, 66, 75]},\n",
        "                                   \"iqr_sec\": float(iqr(timings)),\n",
        "                                   \"skewness\": float(skew(timings)),\n",
        "                                   \"kurtosis\": float(kurtosis(timings))}}\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Results: {json.dumps(results_entry, indent=4)}\")\n",
        "\n",
        "    # Save JSON\n",
        "    if save_path:\n",
        "        try:\n",
        "            if os.path.exists(save_path):\n",
        "                with open(save_path, 'r') as f:\n",
        "                    existing = json.load(f)\n",
        "                if not isinstance(existing, dict):\n",
        "                    existing = {}\n",
        "            else:\n",
        "                existing = {}\n",
        "            existing.update(results_entry)\n",
        "            with open(save_path, 'w') as f:\n",
        "                json.dump(existing, f, indent=4)\n",
        "            if verbose:\n",
        "                print(f\"Saved CUDA Timing to: {save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save CUDA Timing JSON: {e}\")\n",
        "\n",
        "    # Save NPZ\n",
        "    if npz_save_path:\n",
        "        try:\n",
        "            np.savez_compressed(npz_save_path, timings=timings)\n",
        "            if verbose:\n",
        "                print(f\"Saved compressed CUDA Timings to: {npz_save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save CUDA Times NPZ: {e}\")\n",
        "\n",
        "    return results_entry\n",
        "\n",
        "\n",
        "def e2e_inference_time(model, sample_rate, device, save_path, iterations=100, input_duration_sec=10.0, verbose=True, npz_save_path=None):\n",
        "    \"\"\"\n",
        "    Measure End-to-End (CPU+GPU) inference timing.\n",
        "\n",
        "    :param model: Your PyTorch model.\n",
        "    :param sample_rate: Audio sample rate (e.g., 32000).\n",
        "    :param device: Device string ('cpu' or 'cuda').\n",
        "    :param save_path: Path to save the JSON results.\n",
        "    :param iterations: Number of iterations.\n",
        "    :param input_duration_sec: Input duration in seconds.\n",
        "    :param verbose: Print verbose output.\n",
        "    :param npz_save_path: Optional path to save .npz results.\n",
        "    \"\"\"\n",
        "    def generate_input(duration_samples, device):\n",
        "        return torch.randn((1, duration_samples), device=device) * 2 - 1.\n",
        "\n",
        "    samples = int(sample_rate * input_duration_sec)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    timings = []\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        x = generate_input(samples, device)\n",
        "\n",
        "        for _ in tqdm(range(iterations), desc=\"E2E Inference Timing\"):\n",
        "            set_seeds(42)\n",
        "            start = time.perf_counter()\n",
        "            _ = model(x.float())\n",
        "            if device.startswith(\"cuda\"):\n",
        "                torch.cuda.synchronize()\n",
        "            timings.append(time.perf_counter() - start)\n",
        "\n",
        "    timings = np.array(timings)\n",
        "\n",
        "    results_entry = {\"e2e_inference_time\": {\"iterations\": int(iterations),\n",
        "                                            \"input_duration_sec\": float(input_duration_sec),\n",
        "                                            \"max_sec\": float(np.max(timings)),\n",
        "                                            \"min_sec\": float(np.min(timings)),\n",
        "                                            \"mean_sec\": float(np.mean(timings)),\n",
        "                                            \"std_dev_sec\": float(np.std(timings, ddof=1)),\n",
        "                                            \"median_sec\": float(np.median(timings)),\n",
        "                                            \"percentiles\": {f\"{p}th_perc\": float(np.percentile(timings, p)) for p in [25, 33, 66, 75]},\n",
        "                                            \"iqr_sec\": float(iqr(timings)),\n",
        "                                            \"skewness\": float(skew(timings)),\n",
        "                                            \"kurtosis\": float(kurtosis(timings))}}\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Results: {json.dumps(results_entry, indent=4)}\")\n",
        "\n",
        "    if save_path:\n",
        "        try:\n",
        "            if os.path.exists(save_path):\n",
        "                with open(save_path, 'r') as f:\n",
        "                    existing = json.load(f)\n",
        "                if not isinstance(existing, dict):\n",
        "                    existing = {}\n",
        "            else:\n",
        "                existing = {}\n",
        "            existing.update(results_entry)\n",
        "            with open(save_path, 'w') as f:\n",
        "                json.dump(existing, f, indent=4)\n",
        "            if verbose:\n",
        "                print(f\"Saved E2E inference timing to: {save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save E2E timing JSON: {e}\")\n",
        "\n",
        "    if npz_save_path:\n",
        "        try:\n",
        "            np.savez_compressed(npz_save_path, timings=timings)\n",
        "            if verbose:\n",
        "                print(f\"Saved compressed E2E timings to: {npz_save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save E2E NPZ: {e}\")\n",
        "\n",
        "    return results_entry\n",
        "\n",
        "\n",
        "def gpu_memory(model, sample_rate, device, save_path, iterations=100, input_duration_sec=10.0, verbose=True):\n",
        "    \"\"\"\n",
        "    Measure peak GPU memory usage during model inference.\n",
        "\n",
        "    :param model: Your PyTorch model.\n",
        "    :param sample_rate: Audio sample rate (e.g., 32000).\n",
        "    :param device: Device string ('cuda' required).\n",
        "    :param save_path: Path to save the JSON results.\n",
        "    :param iterations: Number of iterations.\n",
        "    :param input_duration_sec: Input duration in seconds.\n",
        "    :param verbose: Print verbose output.\n",
        "    \"\"\"\n",
        "    assert device.startswith(\"cuda\"), \"GPU memory profiling requires a CUDA device.\"\n",
        "\n",
        "    def generate_input(duration_samples, device):\n",
        "        return torch.randn((1, duration_samples), device=device) * 2 - 1.\n",
        "\n",
        "    samples = int(sample_rate * input_duration_sec)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        x = generate_input(samples, device)\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "        for _ in tqdm(range(iterations), desc=\"GPU Memory Usage Profiling\"):\n",
        "            _ = model(x.float())\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "    peak_memory_bytes = torch.cuda.max_memory_allocated()\n",
        "\n",
        "    results_entry = {\"gpu_memory_usage\": {\"iterations\": int(iterations),\n",
        "                                          \"input_duration_sec\": float(input_duration_sec),\n",
        "                                          \"peak_memory_bytes\": int(peak_memory_bytes),\n",
        "                                          \"peak_memory_megabytes\": round(peak_memory_bytes / (1024**2), 4)}}\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Results: {json.dumps(results_entry, indent=4)}\")\n",
        "\n",
        "    if save_path:\n",
        "        try:\n",
        "            if os.path.exists(save_path):\n",
        "                with open(save_path, 'r') as f:\n",
        "                    existing = json.load(f)\n",
        "                if not isinstance(existing, dict):\n",
        "                    existing = {}\n",
        "            else:\n",
        "                existing = {}\n",
        "            existing.update(results_entry)\n",
        "            with open(save_path, 'w') as f:\n",
        "                json.dump(existing, f, indent=4)\n",
        "            if verbose:\n",
        "                print(f\"Saved GPU memory usage to: {save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save GPU memory usage JSON: {e}\")\n",
        "\n",
        "    return results_entry\n",
        "\n",
        "\n",
        "def gpu_usage(model, sample_rate, device, save_path, iterations=100, input_duration_sec=10.0, verbose=True):\n",
        "    \"\"\"\n",
        "    Measure GPU utilization percentage during model inference.\n",
        "\n",
        "    :param model: Your PyTorch model.\n",
        "    :param sample_rate: Audio sample rate (e.g., 32000).\n",
        "    :param device: Device string ('cuda' required).\n",
        "    :param save_path: Path to save the JSON results.\n",
        "    :param iterations: Number of iterations.\n",
        "    :param input_duration_sec: Input duration in seconds.\n",
        "    :param verbose: Print verbose output.\n",
        "    \"\"\"\n",
        "    assert device.startswith(\"cuda\"), \"GPU utilization monitoring requires a CUDA device.\"\n",
        "\n",
        "    def generate_input(duration_samples, device):\n",
        "        return torch.randn((1, duration_samples), device=device) * 2 - 1.\n",
        "\n",
        "    samples = int(sample_rate * input_duration_sec)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    gpu_queue = Queue()\n",
        "    stop_event = threading.Event()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        x = generate_input(samples, device)\n",
        "\n",
        "        monitor_thread = threading.Thread(target=monitor_gpu_usage, args=(gpu_queue, stop_event))\n",
        "        monitor_thread.start()\n",
        "\n",
        "        for _ in tqdm(range(iterations), desc=\"GPU Utilization Profiling\"):\n",
        "            _ = model(x.float())\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        stop_event.set()\n",
        "        monitor_thread.join()\n",
        "\n",
        "    gpu_samples = []\n",
        "    while not gpu_queue.empty():\n",
        "        gpu_samples.append(gpu_queue.get())\n",
        "\n",
        "    avg_gpu = sum(gpu_samples) / len(gpu_samples) if gpu_samples else 0\n",
        "    peak_gpu = max(gpu_samples) if gpu_samples else 0\n",
        "\n",
        "    results_entry = {\"gpu_utilization\": {\"iterations\": int(iterations),\n",
        "                                         \"input_duration_sec\": float(input_duration_sec),\n",
        "                                         \"avg_utilization_percent\": float(avg_gpu),\n",
        "                                         \"peak_utilization_percent\": float(peak_gpu)}}\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Results: {json.dumps(results_entry, indent=4)}\")\n",
        "\n",
        "    if save_path:\n",
        "        try:\n",
        "            if os.path.exists(save_path):\n",
        "                with open(save_path, 'r') as f:\n",
        "                    existing = json.load(f)\n",
        "                if not isinstance(existing, dict):\n",
        "                    existing = {}\n",
        "            else:\n",
        "                existing = {}\n",
        "            existing.update(results_entry)\n",
        "            with open(save_path, 'w') as f:\n",
        "                json.dump(existing, f, indent=4)\n",
        "            if verbose:\n",
        "                print(f\"Saved GPU utilization to: {save_path}\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Failed to save GPU utilization JSON: {e}\")\n",
        "\n",
        "    return results_entry\n"
      ],
      "metadata": {
        "id": "vbRHG4LtBvkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture"
      ],
      "metadata": {
        "id": "u1Zc8PNqOUI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "!pip install torchprofile\n",
        "!pip install torchview\n",
        "!pip install loguru\n",
        "\n",
        "from torchinfo import summary\n",
        "from torchprofile import profile_macs\n",
        "from torchview import draw_graph\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "from loguru import logger"
      ],
      "metadata": {
        "id": "vWuf9RL-OYWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66965ae6-57e2-44c1-80cd-8a0ef7a7a3bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "Collecting torchprofile\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.11/dist-packages (from torchprofile) (2.0.2)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.11/dist-packages (from torchprofile) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.4 in /usr/local/lib/python3.11/dist-packages (from torchprofile) (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->torchprofile) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4->torchprofile) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.4->torchprofile) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4->torchprofile) (3.0.2)\n",
            "Downloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Installing collected packages: torchprofile\n",
            "Successfully installed torchprofile-0.0.4\n",
            "Collecting torchview\n",
            "  Downloading torchview-0.2.6-py3-none-any.whl.metadata (12 kB)\n",
            "Downloading torchview-0.2.6-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: torchview\n",
            "Successfully installed torchview-0.2.6\n",
            "Collecting loguru\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "\n",
        "# Lightning-2-PyTorch Checkpoints Loader ----------------------------------------------------------------------\n",
        "def load_lightning2pt(checkpoint_path, model, device=\"cpu\", verbose=True, validate_updates=True):\n",
        "    \"\"\"\n",
        "    Loads a PyTorch Lightning checkpoint's state_dict into a plain PyTorch model and optionally verifies parameter updates.\n",
        "\n",
        "    :param checkpoint_path: Absolute Path to the Lightning checkpoint file (.ckpt).\n",
        "    :param model: The plain PyTorch model instance to load the checkpoint into.\n",
        "    :param device: Device to load the model onto ('cpu' or 'cuda').\n",
        "    :param verbose: Whether to print detailed information about the loading process (default: True).\n",
        "    :param validate_updates: Whether to validate which layers were updated during fine-tuning (default: True).\n",
        "    :return: The plain PyTorch model with weights loaded from the checkpoint, and a list of updated layers (if validated).\n",
        "    \"\"\"\n",
        "    # Step 1: Load the Lightning checkpoint\n",
        "    try:\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    except FileNotFoundError:\n",
        "        raise ValueError(f\"Checkpoint file not found at: {checkpoint_path}\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to load checkpoint: {e}\")\n",
        "\n",
        "    # Step 2: Extract the Lightning state_dict\n",
        "    if \"state_dict\" not in checkpoint:\n",
        "        raise ValueError(f\"Checkpoint does not contain a 'state_dict'. Keys found: {list(checkpoint.keys())}\")\n",
        "\n",
        "    lightning_state_dict = checkpoint[\"state_dict\"]\n",
        "\n",
        "    # Step 3: Generalize prefix removal\n",
        "    stripped_state_dict = {}\n",
        "    prefix = None\n",
        "\n",
        "    for key in lightning_state_dict.keys():\n",
        "        if \".\" in key:\n",
        "            prefix = key.split(\".\")[0] + \".\"\n",
        "            break\n",
        "\n",
        "    if prefix:\n",
        "        stripped_state_dict = {key.replace(prefix, \"\"): value for key, value in lightning_state_dict.items()}\n",
        "        if verbose:\n",
        "            print(f\"Detected prefix '{prefix}'. Stripped from state_dict keys.\")\n",
        "    else:\n",
        "        stripped_state_dict = lightning_state_dict\n",
        "        if verbose:\n",
        "            print(\"No prefix detected in state_dict keys.\")\n",
        "\n",
        "    # Step 4: Move the model to the specified device\n",
        "    model.to(device)\n",
        "    if verbose:\n",
        "        print(f\"Model moved to device: {device}\")\n",
        "\n",
        "    # Step 5: Optionally validate parameter updates\n",
        "    updated_layers = []\n",
        "    if validate_updates:\n",
        "        for name, param in model.state_dict().items():\n",
        "            if name in stripped_state_dict:\n",
        "                old_param = param.clone()\n",
        "                new_param = stripped_state_dict[name]\n",
        "\n",
        "                # Print data type information\n",
        "                if verbose:\n",
        "                    print(f\"Validating layer: {name}\")\n",
        "                    print(f\"  Old Param: Type: {type(old_param)}, DType: {old_param.dtype}\")\n",
        "                    print(f\"  New Param: Type: {type(new_param)}, DType: {new_param.dtype}\")\n",
        "\n",
        "                # Compare old and new parameters\n",
        "                if not torch.equal(old_param, new_param):\n",
        "                    updated_layers.append(name)\n",
        "\n",
        "                    # Compute and display parameter differences\n",
        "                    diff = (old_param - new_param).float()\n",
        "                    if verbose:\n",
        "                        print(f\"  Layer: {name} has changes!\")\n",
        "                        print(f\"    Min Difference: {diff.abs().min().item():.6f}\")\n",
        "                        print(f\"    Max Difference: {diff.abs().max().item():.6f}\")\n",
        "                        print(f\"    Mean Difference: {diff.abs().mean().item():.6f}\")\n",
        "                        print(f\"    Std-Dev of Differences: {diff.abs().std().item():.6f}\")\n",
        "\n",
        "                        # Optionally, display a small set of differences\n",
        "                        print(f\"    Sample Differences: {diff.flatten()[:5].tolist()}...\")\n",
        "                print('---------------------------------------------------------------------------------')\n",
        "\n",
        "    # Load the stripped state_dict into the plain model\n",
        "    try:\n",
        "        model.load_state_dict(stripped_state_dict)\n",
        "        if verbose:\n",
        "            print(\"State dict successfully loaded into the model!\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to load state_dict into the model: {e}\")\n",
        "\n",
        "    # Step 6: Print updated layers if validated\n",
        "    if verbose and validate_updates:\n",
        "        if updated_layers:\n",
        "            print(\"The following layers were updated during fine-tuning:\")\n",
        "            for layer in updated_layers:\n",
        "                print(f\" - {layer}\")\n",
        "        else:\n",
        "            print(\"No layers were updated. Fine-tuning may not have modified the model.\")\n",
        "\n",
        "    # Return the model and optionally updated layers\n",
        "    return model, updated_layers if validate_updates else None\n",
        "\n",
        "\n",
        "# Model profiling functions ----------------------------------------------------------------------\n",
        "def inference_trace(model, sample_rate, input_duration_sec=10.0, device=\"cpu\", save_path=None):\n",
        "    \"\"\"\n",
        "    Perform an inference trace profiling of the model using PyTorch's profiler.\n",
        "    Saves a Chrome Trace JSON file containing CPU and CUDA activity traces.\n",
        "\n",
        "    :param model: The PyTorch model to profile.\n",
        "    :param sample_rate: Audio sample rate (e.g., 32000).\n",
        "    :param input_duration_sec: Duration of input audio in seconds (default: 10.0).\n",
        "    :param device: Device to run the model on ('cpu' or 'cuda').\n",
        "    :param save_path: Path to save the Chrome trace JSON file (required).\n",
        "    :param verbose: Whether to print progress messages (default: True).\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    if save_path is None:\n",
        "        raise ValueError(\"You must provide a save_path to store the Chrome trace JSON file.\")\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Generate random input tensor\n",
        "    num_samples = int(sample_rate * input_duration_sec)\n",
        "    x = torch.randn((1, num_samples), device=device) * 2 - 1.\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "                     profile_memory=True,\n",
        "                     record_shapes=True) as prof:\n",
        "            with record_function(\"model_inference\"):\n",
        "                _ = model(x.float())\n",
        "\n",
        "    try:\n",
        "        prof.export_chrome_trace(save_path)\n",
        "        print(f\"Inference trace successfully saved to: {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save inference trace: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def architecture_profile(model, sample_rate, input_duration_sec=10.0, device=\"cpu\", save_path=None):\n",
        "    \"\"\"\n",
        "    Perform full architecture profiling of the model:\n",
        "    - Structure summary using torchinfo\n",
        "    - MACs and FLOPs estimation using torchprofile\n",
        "    Saves all results to a dedicated log file using Loguru.\n",
        "\n",
        "    :param model: The PyTorch model to profile.\n",
        "    :param sample_rate: Audio sample rate (e.g., 32000).\n",
        "    :param input_duration_sec: Duration of input audio in seconds (default: 10.0).\n",
        "    :param device: Device to run the model on ('cpu' or 'cuda').\n",
        "    :param save_path: Path to save the full architecture summary log file (required).\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    if save_path is None:\n",
        "        raise ValueError(\"You must provide a save_path to store the architecture summary log.\")\n",
        "\n",
        "    # Step 1: Model preparation\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Generate input shape and input tensor\n",
        "    num_samples = int(sample_rate * input_duration_sec)\n",
        "    input_shape = (1, num_samples)\n",
        "    dummy_input = torch.randn((1, num_samples), device='cpu') * 2 - 1.\n",
        "\n",
        "    try:\n",
        "        # Create a dedicated logger\n",
        "        loguru_logger = logger.bind(name=\"architecture_profile\")\n",
        "        loguru_logger.add(save_path, format=\"{message}\", level=\"INFO\", enqueue=True)\n",
        "\n",
        "        # Step 2: Architecture Summary\n",
        "        model_stats = summary(model=model,\n",
        "                              input_size=input_shape,\n",
        "                              cache_forward_pass=True,\n",
        "                              col_names=(\"input_size\",\n",
        "                                         \"output_size\",\n",
        "                                         \"num_params\",\n",
        "                                         \"params_percent\",\n",
        "                                         \"kernel_size\",\n",
        "                                         \"mult_adds\",\n",
        "                                         \"trainable\"),\n",
        "                              depth=100,\n",
        "                              device=device,\n",
        "                              row_settings=(\"ascii_only\",\n",
        "                                            \"depth\",\n",
        "                                            \"var_names\"),\n",
        "                              verbose=0)\n",
        "\n",
        "        summary_str = str(model_stats)\n",
        "        loguru_logger.info(summary_str)\n",
        "\n",
        "        # Step 3: MACs and FLOPs Estimation\n",
        "        mul_add_cum = profile_macs(model.cpu(), dummy_input)\n",
        "\n",
        "        loguru_logger.info(f\"MACs    : {mul_add_cum}\")\n",
        "        loguru_logger.info(f\"FLOPs   : {mul_add_cum * 2}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed during full architecture profiling: {e}\")"
      ],
      "metadata": {
        "id": "kaLakqAOSNAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Profiling"
      ],
      "metadata": {
        "id": "tzej1hRXB1u4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove old RunTimes Directory\n",
        "!rm -rf 3_profiling_results_COLAB"
      ],
      "metadata": {
        "id": "8_At0x_eECBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from epanns_inference import models\n",
        "\n",
        "# Ensure reproducibility\n",
        "set_seeds(42)\n",
        "\n",
        "# Output Parameters -----------------------------------------------------------------------------\n",
        "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "RESULTS_DIR = \"./3_profiling_results_COLAB\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "DuJ5IrG5B6Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU & Hardware profiling ----------------------------------------------------------------------\n",
        "gpu_info_path = os.path.join(RESULTS_DIR, f\"gpu_info_{TIMESTAMP}.json\")\n",
        "device, gpu_info = your_gpu(verbose=True, save_path=gpu_info_path)\n",
        "print(f\"Torch device: {device}\")\n",
        "print('\\n')\n",
        "\n",
        "hardware_info_path = os.path.join(RESULTS_DIR, f\"hardware_info_{TIMESTAMP}.json\")\n",
        "hardware_info = your_hardware(verbose=True, save_path=hardware_info_path)\n",
        "print('\\n')\n",
        "\n",
        "# CPU inference profiling -----------------------------------------------------------------------\n",
        "SAMPLE_RATE = 32000\n",
        "INPUT_DURATION_SEC = 10.0\n",
        "ITERATIONS = 100\n",
        "CHECKPOINT_PATH = \"./multi-unified.ckpt\"\n",
        "\n",
        "# Load the model\n",
        "model = models.Cnn14_pruned(pre_trained=False)\n",
        "model, _ = load_lightning2pt(CHECKPOINT_PATH, model, device=\"cpu\", verbose=True, validate_updates=False)\n",
        "print('\\n')\n",
        "model.cpu()\n",
        "model_info_path = os.path.join(RESULTS_DIR, f\"model_info_{TIMESTAMP}.json\")\n",
        "\n",
        "# Minimum inpout size\n",
        "min_binary_search(model=model,\n",
        "                  sample_rate=SAMPLE_RATE,\n",
        "                  device=\"cpu\",\n",
        "                  save_path=model_info_path,\n",
        "                  verbose=True)\n",
        "print('\\n')\n",
        "\n",
        "# Benchmark CPU Overall Time\n",
        "npz_path = os.path.join(RESULTS_DIR, f\"cpu_overall_times_{TIMESTAMP}.npz\")\n",
        "overall_time(model=model,\n",
        "             device=\"cpu\",\n",
        "             sample_rate=SAMPLE_RATE,\n",
        "             input_duration_sec=INPUT_DURATION_SEC,\n",
        "             iterations=ITERATIONS,\n",
        "             verbose=True,\n",
        "             save_path=model_info_path,\n",
        "             npz_save_path=npz_path)\n",
        "print('\\n')\n",
        "\n",
        "# CPU Process Time\n",
        "npz_path = os.path.join(RESULTS_DIR, f\"cpu_process_times_{TIMESTAMP}.npz\")\n",
        "process_time(model=model,\n",
        "             device=\"cpu\",\n",
        "             sample_rate=SAMPLE_RATE,\n",
        "             input_duration_sec=INPUT_DURATION_SEC,\n",
        "             iterations=ITERATIONS,\n",
        "             verbose=True,\n",
        "             save_path=model_info_path,\n",
        "             npz_save_path=npz_path)\n",
        "print('\\n')\n",
        "\n",
        "# Memory/Cache Usage\n",
        "npz_path = os.path.join(RESULTS_DIR, f\"memory_and_cache_{TIMESTAMP}.npz\")\n",
        "memory(model=model,\n",
        "       device=\"cpu\",\n",
        "       sample_rate=SAMPLE_RATE,\n",
        "       input_duration_sec=INPUT_DURATION_SEC,\n",
        "       iterations=ITERATIONS,\n",
        "       verbose=True,\n",
        "       save_path=model_info_path,\n",
        "       npz_save_path=npz_path)\n",
        "print('\\n')\n",
        "\n",
        "# CPU Usage\n",
        "npz_path = os.path.join(RESULTS_DIR, f\"cpu_usage_{TIMESTAMP}.npz\")\n",
        "cpu_usage(model=model,\n",
        "          device=\"cpu\",\n",
        "          sample_rate=SAMPLE_RATE,\n",
        "          input_duration_sec=INPUT_DURATION_SEC,\n",
        "          iterations=ITERATIONS,\n",
        "          verbose=True,\n",
        "          save_path=model_info_path,\n",
        "          npz_save_path=npz_path)\n",
        "print('\\n')\n",
        "\n",
        "# GPU Inference Profiling -------------------------------------------------------------------\n",
        "model.cuda()\n",
        "\n",
        "# Cuda Time\n",
        "npz_path = os.path.join(RESULTS_DIR, f\"cuda_times_{TIMESTAMP}.npz\")\n",
        "cuda_time(model=model,\n",
        "          device=\"cuda:0\",\n",
        "          sample_rate=SAMPLE_RATE,\n",
        "          input_duration_sec=INPUT_DURATION_SEC,\n",
        "          iterations=ITERATIONS,\n",
        "          verbose=True,\n",
        "          save_path=model_info_path,\n",
        "          npz_save_path=npz_path)\n",
        "print('\\n')\n",
        "\n",
        "# E2E Inference Time\n",
        "npz_path = os.path.join(RESULTS_DIR, f\"e2e_inference_times_{TIMESTAMP}.npz\")\n",
        "e2e_inference_time(model=model,\n",
        "                   device=\"cuda:0\",\n",
        "                   sample_rate=SAMPLE_RATE,\n",
        "                   input_duration_sec=INPUT_DURATION_SEC,\n",
        "                   iterations=ITERATIONS,\n",
        "                   verbose=True,\n",
        "                   save_path=model_info_path,\n",
        "                   npz_save_path=npz_path)\n",
        "print('\\n')\n",
        "\n",
        "# GPU Memory Usage\n",
        "gpu_memory(model=model,\n",
        "           device=\"cuda:0\",\n",
        "           sample_rate=SAMPLE_RATE,\n",
        "           input_duration_sec=INPUT_DURATION_SEC,\n",
        "           iterations=ITERATIONS,\n",
        "           verbose=True,\n",
        "           save_path=model_info_path)\n",
        "print('\\n')\n",
        "\n",
        "# Cuda-processors Usage\n",
        "gpu_usage(model=model,\n",
        "          device=\"cuda:0\",\n",
        "          sample_rate=SAMPLE_RATE,\n",
        "          input_duration_sec=INPUT_DURATION_SEC,\n",
        "          iterations=ITERATIONS,\n",
        "          verbose=True,\n",
        "          save_path=model_info_path)\n",
        "print('\\n')\n",
        "\n",
        "# Energy and CO2 Emissions\n",
        "npz_path = os.path.join(RESULTS_DIR, f\"energy_co2_{TIMESTAMP}.npz\")\n",
        "energy_co2(model=model,\n",
        "           device=\"cpu\",\n",
        "           sample_rate=SAMPLE_RATE,\n",
        "           input_duration_sec=INPUT_DURATION_SEC,\n",
        "           iterations=ITERATIONS,\n",
        "           verbose=True,\n",
        "           save_path=model_info_path,\n",
        "           npz_save_path=npz_path)\n",
        "print('\\n')\n",
        "\n",
        "print(\"EOF\")"
      ],
      "metadata": {
        "id": "pK1KrwHDCowq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45baa0e2-0521-4809-8155-130bbc06ca30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GPU-0] Name: Tesla T4, Free: 15.72 GB, Total: 15.83 GB\n",
            "Driver Version: 550.54.15\n",
            "CUDA Compiler Version: 12.5\n",
            "PyTorch Version: 2.6.0+cu124\n",
            "Saved GPU info JSON to: ./3_profiling_results_COLAB/gpu_info_20250409_1028.json\n",
            "Torch device: cuda:0\n",
            "\n",
            "\n",
            "CPU Info retrieved from /proc/cpuinfo\n",
            "Hardware Summary: {\n",
            "    \"cpu\": {\n",
            "        \"model_name\": \"Intel(R) Xeon(R) CPU @ 2.30GHz\",\n",
            "        \"physical_cores\": 8,\n",
            "        \"frequencies_mhz\": {\n",
            "            \"cpu0\": 0.0,\n",
            "            \"cpu1\": 0.0,\n",
            "            \"cpu2\": 0.0,\n",
            "            \"cpu3\": 0.0,\n",
            "            \"cpu4\": 0.0,\n",
            "            \"cpu5\": 0.0,\n",
            "            \"cpu6\": 0.0,\n",
            "            \"cpu7\": 0.0\n",
            "        }\n",
            "    },\n",
            "    \"ram\": {\n",
            "        \"total_memory_gb\": 54.75,\n",
            "        \"available_memory_gb\": 51.95,\n",
            "        \"used_memory_gb\": 2.15,\n",
            "        \"percent_used\": 5.1\n",
            "    },\n",
            "    \"disks\": [\n",
            "        {\n",
            "            \"Filesystem\": \"overlay\",\n",
            "            \"Size\": \"236G\",\n",
            "            \"Used\": \"41G\",\n",
            "            \"Avail\": \"195G\",\n",
            "            \"Use%\": \"18%\",\n",
            "            \"Mounted\": \"/\"\n",
            "        },\n",
            "        {\n",
            "            \"Filesystem\": \"tmpfs\",\n",
            "            \"Size\": \"64M\",\n",
            "            \"Used\": \"0\",\n",
            "            \"Avail\": \"64M\",\n",
            "            \"Use%\": \"0%\",\n",
            "            \"Mounted\": \"/dev\"\n",
            "        },\n",
            "        {\n",
            "            \"Filesystem\": \"shm\",\n",
            "            \"Size\": \"25G\",\n",
            "            \"Used\": \"0\",\n",
            "            \"Avail\": \"25G\",\n",
            "            \"Use%\": \"0%\",\n",
            "            \"Mounted\": \"/dev/shm\"\n",
            "        },\n",
            "        {\n",
            "            \"Filesystem\": \"/dev/root\",\n",
            "            \"Size\": \"2.0G\",\n",
            "            \"Used\": \"1.2G\",\n",
            "            \"Avail\": \"820M\",\n",
            "            \"Use%\": \"59%\",\n",
            "            \"Mounted\": \"/usr/sbin/docker-init\"\n",
            "        },\n",
            "        {\n",
            "            \"Filesystem\": \"/dev/sda1\",\n",
            "            \"Size\": \"242G\",\n",
            "            \"Used\": \"70G\",\n",
            "            \"Avail\": \"173G\",\n",
            "            \"Use%\": \"29%\",\n",
            "            \"Mounted\": \"/opt/bin/.nvidia\"\n",
            "        },\n",
            "        {\n",
            "            \"Filesystem\": \"tmpfs\",\n",
            "            \"Size\": \"26G\",\n",
            "            \"Used\": \"2.1M\",\n",
            "            \"Avail\": \"26G\",\n",
            "            \"Use%\": \"1%\",\n",
            "            \"Mounted\": \"/var/colab\"\n",
            "        },\n",
            "        {\n",
            "            \"Filesystem\": \"tmpfs\",\n",
            "            \"Size\": \"26G\",\n",
            "            \"Used\": \"0\",\n",
            "            \"Avail\": \"26G\",\n",
            "            \"Use%\": \"0%\",\n",
            "            \"Mounted\": \"/proc/acpi\"\n",
            "        },\n",
            "        {\n",
            "            \"Filesystem\": \"tmpfs\",\n",
            "            \"Size\": \"26G\",\n",
            "            \"Used\": \"0\",\n",
            "            \"Avail\": \"26G\",\n",
            "            \"Use%\": \"0%\",\n",
            "            \"Mounted\": \"/proc/scsi\"\n",
            "        },\n",
            "        {\n",
            "            \"Filesystem\": \"tmpfs\",\n",
            "            \"Size\": \"26G\",\n",
            "            \"Used\": \"0\",\n",
            "            \"Avail\": \"26G\",\n",
            "            \"Use%\": \"0%\",\n",
            "            \"Mounted\": \"/sys/firmware\"\n",
            "        },\n",
            "        {\n",
            "            \"Filesystem\": \"drive\",\n",
            "            \"Size\": \"15G\",\n",
            "            \"Used\": \"4.8G\",\n",
            "            \"Avail\": \"11G\",\n",
            "            \"Use%\": \"32%\",\n",
            "            \"Mounted\": \"/content/drive\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "Saved hardware info JSON to: ./3_profiling_results_COLAB/hardware_info_20250409_1028.json\n",
            "\n",
            "\n",
            "Detected prefix 'model.'. Stripped from state_dict keys.\n",
            "Model moved to device: cpu\n",
            "State dict successfully loaded into the model!\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MIN Input Size Binary Search:   0%|          | 0/320000 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/epanns_inference/models/models.py:241: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  clipwise_output = nn.functional.softmax(self.fc_audioset(x))\n",
            "MIN Input Size Binary Search: 100%|█████████▉| 319998/320000 [00:00<00:00, 423413.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: {\n",
            "    \"min_input_size\": {\n",
            "        \"samples\": 9919,\n",
            "        \"seconds\": 0.30996875,\n",
            "        \"sample_rate\": 32000,\n",
            "        \"binary_search_iterations\": 19\n",
            "    }\n",
            "}\n",
            "Saved (updated) profiling results to: ./3_profiling_results_COLAB/model_info_20250409_1028.json\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CPU Overall Time Profiling: 100%|██████████| 100/100 [00:22<00:00,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: {\n",
            "    \"cpu_overall_time\": {\n",
            "        \"iterations\": 100,\n",
            "        \"input_duration_sec\": 10.0,\n",
            "        \"max_sec\": 0.31162175100007516,\n",
            "        \"min_sec\": 0.18841470300003493,\n",
            "        \"mean_sec\": 0.21822004696998876,\n",
            "        \"std_dev_sec\": 0.036087429297872085,\n",
            "        \"median_sec\": 0.20275976699991816,\n",
            "        \"percentiles\": {\n",
            "            \"25th_perc\": 0.19719780199997672,\n",
            "            \"33th_perc\": 0.19843507044994113,\n",
            "            \"66th_perc\": 0.20985839805997786,\n",
            "            \"75th_perc\": 0.21594237100001124\n",
            "        },\n",
            "        \"iqr_sec\": 0.018744569000034517,\n",
            "        \"skewness\": 1.6676633104478196,\n",
            "        \"kurtosis\": 1.2622859354801985\n",
            "    }\n",
            "}\n",
            "Saved (updated) profiling results to: ./3_profiling_results_COLAB/model_info_20250409_1028.json\n",
            "Saved compressed NPZ data to: ./3_profiling_results_COLAB/cpu_overall_times_20250409_1028.npz\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CPU Process Time Profiling: 100%|██████████| 100/100 [00:21<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: {\n",
            "    \"cpu_process_time\": {\n",
            "        \"iterations\": 100,\n",
            "        \"input_duration_sec\": 10.0,\n",
            "        \"max_sec\": 1.2924306919999964,\n",
            "        \"min_sec\": 0.7234315620000018,\n",
            "        \"mean_sec\": 0.8216555288399979,\n",
            "        \"std_dev_sec\": 0.13417356655347595,\n",
            "        \"median_sec\": 0.7606123329999974,\n",
            "        \"percentiles\": {\n",
            "            \"25th_perc\": 0.7417675297500068,\n",
            "            \"33th_perc\": 0.7449946447599959,\n",
            "            \"66th_perc\": 0.7910177704200043,\n",
            "            \"75th_perc\": 0.8145932839999972\n",
            "        },\n",
            "        \"iqr_sec\": 0.07282575424999038,\n",
            "        \"skewness\": 1.8209846660259033,\n",
            "        \"kurtosis\": 2.2026931938970673\n",
            "    }\n",
            "}\n",
            "Saved (updated) profiling results to: ./3_profiling_results_COLAB/model_info_20250409_1028.json\n",
            "Saved compressed NPZ data to: ./3_profiling_results_COLAB/cpu_process_times_20250409_1028.npz\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Memory Profiling Only: 100%|██████████| 100/100 [00:21<00:00,  4.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"memory_usage\": {\n",
            "        \"iterations\": 100,\n",
            "        \"input_duration_sec\": 10.0,\n",
            "        \"current_bytes\": 44194,\n",
            "        \"peak_bytes\": 102487,\n",
            "        \"current_megabytes\": 0.0421,\n",
            "        \"peak_megabytes\": 0.0977\n",
            "    }\n",
            "}\n",
            "Saved (updated) memory profiling results to: ./3_profiling_results_COLAB/model_info_20250409_1028.json\n",
            "Saved compressed NPZ memory data to: ./3_profiling_results_COLAB/memory_and_cache_20250409_1028.npz\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CPU Usage Profiling: 100%|██████████| 100/100 [00:20<00:00,  4.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: {\n",
            "    \"cpu_usage\": {\n",
            "        \"iterations\": 100,\n",
            "        \"input_duration_sec\": 10.0,\n",
            "        \"avg_cpu_usage_percent\": 50.63,\n",
            "        \"peak_cpu_usage_percent\": 69.6\n",
            "    }\n",
            "}\n",
            "Saved (updated) CPU usage results to: ./3_profiling_results_COLAB/model_info_20250409_1028.json\n",
            "Saved compressed NPZ CPU usage data to: ./3_profiling_results_COLAB/cpu_usage_20250409_1028.npz\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CUDA Event Timing: 100%|██████████| 100/100 [00:01<00:00, 60.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: {\n",
            "    \"cuda_time\": {\n",
            "        \"iterations\": 100,\n",
            "        \"input_duration_sec\": 10.0,\n",
            "        \"max_sec\": 0.49310906982421876,\n",
            "        \"min_sec\": 0.009578495979309083,\n",
            "        \"mean_sec\": 0.015809169750213622,\n",
            "        \"std_dev_sec\": 0.04829954901922595,\n",
            "        \"median_sec\": 0.009829264163970947,\n",
            "        \"percentiles\": {\n",
            "            \"25th_perc\": 0.009715607643127441,\n",
            "            \"33th_perc\": 0.009742387619018555,\n",
            "            \"66th_perc\": 0.009916383953094483,\n",
            "            \"75th_perc\": 0.009949087858200072\n",
            "        },\n",
            "        \"iqr_sec\": 0.00023348021507263103,\n",
            "        \"skewness\": 9.795335394297489,\n",
            "        \"kurtosis\": 94.3024127075387\n",
            "    }\n",
            "}\n",
            "Saved CUDA Timing to: ./3_profiling_results_COLAB/model_info_20250409_1028.json\n",
            "Saved compressed CUDA Timings to: ./3_profiling_results_COLAB/cuda_times_20250409_1028.npz\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E2E Inference Timing: 100%|██████████| 100/100 [00:01<00:00, 94.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: {\n",
            "    \"e2e_inference_time\": {\n",
            "        \"iterations\": 100,\n",
            "        \"input_duration_sec\": 10.0,\n",
            "        \"max_sec\": 0.010833442000148352,\n",
            "        \"min_sec\": 0.009124144999987038,\n",
            "        \"mean_sec\": 0.009944103550010368,\n",
            "        \"std_dev_sec\": 0.0002961851210873841,\n",
            "        \"median_sec\": 0.009914098500075852,\n",
            "        \"percentiles\": {\n",
            "            \"25th_perc\": 0.0097725325001079,\n",
            "            \"33th_perc\": 0.009835762840061761,\n",
            "            \"66th_perc\": 0.009992997080130408,\n",
            "            \"75th_perc\": 0.010020846999964306\n",
            "        },\n",
            "        \"iqr_sec\": 0.00024831449985640575,\n",
            "        \"skewness\": 0.4154733523589719,\n",
            "        \"kurtosis\": 1.9081065156204513\n",
            "    }\n",
            "}\n",
            "Saved E2E inference timing to: ./3_profiling_results_COLAB/model_info_20250409_1028.json\n",
            "Saved compressed E2E timings to: ./3_profiling_results_COLAB/e2e_inference_times_20250409_1028.npz\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPU Memory Usage Profiling: 100%|██████████| 100/100 [00:01<00:00, 97.88it/s]\n",
            "Exception in thread Thread-11 (monitor_gpu_usage):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: {\n",
            "    \"gpu_memory_usage\": {\n",
            "        \"iterations\": 100,\n",
            "        \"input_duration_sec\": 10.0,\n",
            "        \"peak_memory_bytes\": 212500480,\n",
            "        \"peak_memory_megabytes\": 202.6562\n",
            "    }\n",
            "}\n",
            "Saved GPU memory usage to: ./3_profiling_results_COLAB/model_info_20250409_1028.json\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "TypeError: monitor_gpu_usage() takes 0 positional arguments but 2 were given\n",
            "GPU Utilization Profiling: 100%|██████████| 100/100 [00:01<00:00, 98.14it/s]\n",
            "[codecarbon WARNING @ 10:32:22] Invalid gpu_ids format. Expected a string or a list of ints.\n",
            "[codecarbon INFO @ 10:32:22] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 10:32:22] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 10:32:22] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 10:32:22] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 10:32:22] No CPU tracking mode found. Falling back on CPU constant mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: {\n",
            "    \"gpu_utilization\": {\n",
            "        \"iterations\": 100,\n",
            "        \"input_duration_sec\": 10.0,\n",
            "        \"avg_utilization_percent\": 0.0,\n",
            "        \"peak_utilization_percent\": 0.0\n",
            "    }\n",
            "}\n",
            "Saved GPU utilization to: ./3_profiling_results_COLAB/model_info_20250409_1028.json\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 10:32:23] We saw that you have a Intel(R) Xeon(R) CPU @ 2.30GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 10:32:23] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "[codecarbon INFO @ 10:32:23] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 10:32:23]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 10:32:23]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 10:32:23]   CodeCarbon version: 2.4.2\n",
            "[codecarbon INFO @ 10:32:23]   Available RAM : 50.994 GB\n",
            "[codecarbon INFO @ 10:32:23]   CPU count: 8\n",
            "[codecarbon INFO @ 10:32:23]   CPU model: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "[codecarbon INFO @ 10:32:23]   GPU count: 1\n",
            "[codecarbon INFO @ 10:32:23]   GPU model: 1 x Tesla T4\n",
            "Energy/CO₂ Emissions Profiling:   0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/epanns_inference/models/models.py:241: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  clipwise_output = nn.functional.softmax(self.fc_audioset(x))\n",
            "[codecarbon INFO @ 10:32:24] Energy consumed for RAM : 0.000001 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:24] Energy consumed for all GPUs : 0.000002 kWh. Total GPU Power : 37.100613280476075 W\n",
            "[codecarbon INFO @ 10:32:24] Energy consumed for all CPUs : 0.000002 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:24] 0.000006 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:   1%|          | 1/100 [00:00<00:20,  4.84it/s][codecarbon INFO @ 10:32:24] Energy consumed for RAM : 0.000002 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:24] Energy consumed for all GPUs : 0.000004 kWh. Total GPU Power : 37.38563430074888 W\n",
            "[codecarbon INFO @ 10:32:24] Energy consumed for all CPUs : 0.000005 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:24] 0.000011 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:   2%|▏         | 2/100 [00:00<00:20,  4.86it/s][codecarbon INFO @ 10:32:24] Energy consumed for RAM : 0.000003 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:24] Energy consumed for all GPUs : 0.000006 kWh. Total GPU Power : 31.769953996132674 W\n",
            "[codecarbon INFO @ 10:32:24] Energy consumed for all CPUs : 0.000007 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:24] 0.000016 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:   3%|▎         | 3/100 [00:00<00:19,  4.85it/s][codecarbon INFO @ 10:32:24] Energy consumed for RAM : 0.000004 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:24] Energy consumed for all GPUs : 0.000008 kWh. Total GPU Power : 28.83089822758985 W\n",
            "[codecarbon INFO @ 10:32:24] Energy consumed for all CPUs : 0.000010 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:24] 0.000022 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:   4%|▍         | 4/100 [00:00<00:20,  4.79it/s][codecarbon INFO @ 10:32:25] Energy consumed for RAM : 0.000005 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:25] Energy consumed for all GPUs : 0.000009 kWh. Total GPU Power : 28.506672088382274 W\n",
            "[codecarbon INFO @ 10:32:25] Energy consumed for all CPUs : 0.000012 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:25] 0.000027 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:   5%|▌         | 5/100 [00:01<00:20,  4.73it/s][codecarbon INFO @ 10:32:25] Energy consumed for RAM : 0.000007 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:25] Energy consumed for all GPUs : 0.000011 kWh. Total GPU Power : 26.730961560929504 W\n",
            "[codecarbon INFO @ 10:32:25] Energy consumed for all CPUs : 0.000015 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:25] 0.000032 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:   6%|▌         | 6/100 [00:01<00:20,  4.59it/s][codecarbon INFO @ 10:32:25] Energy consumed for RAM : 0.000008 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:25] Energy consumed for all GPUs : 0.000013 kWh. Total GPU Power : 30.056801578589265 W\n",
            "[codecarbon INFO @ 10:32:25] Energy consumed for all CPUs : 0.000017 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:25] 0.000038 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:   7%|▋         | 7/100 [00:01<00:19,  4.68it/s][codecarbon INFO @ 10:32:25] Energy consumed for RAM : 0.000009 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:25] Energy consumed for all GPUs : 0.000014 kWh. Total GPU Power : 29.112900683600383 W\n",
            "[codecarbon INFO @ 10:32:25] Energy consumed for all CPUs : 0.000020 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:25] 0.000043 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:   8%|▊         | 8/100 [00:01<00:19,  4.70it/s][codecarbon INFO @ 10:32:25] Energy consumed for RAM : 0.000010 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:25] Energy consumed for all GPUs : 0.000015 kWh. Total GPU Power : 15.206313658016827 W\n",
            "[codecarbon INFO @ 10:32:25] Energy consumed for all CPUs : 0.000022 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:25] 0.000047 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:   9%|▉         | 9/100 [00:01<00:19,  4.78it/s][codecarbon INFO @ 10:32:26] Energy consumed for RAM : 0.000011 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:26] Energy consumed for all GPUs : 0.000017 kWh. Total GPU Power : 28.344181854934494 W\n",
            "[codecarbon INFO @ 10:32:26] Energy consumed for all CPUs : 0.000025 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:26] 0.000052 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  10%|█         | 10/100 [00:02<00:19,  4.74it/s][codecarbon INFO @ 10:32:26] Energy consumed for RAM : 0.000012 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:26] Energy consumed for all GPUs : 0.000018 kWh. Total GPU Power : 28.801491918870003 W\n",
            "[codecarbon INFO @ 10:32:26] Energy consumed for all CPUs : 0.000027 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:26] 0.000058 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  11%|█         | 11/100 [00:02<00:18,  4.72it/s][codecarbon INFO @ 10:32:26] Energy consumed for RAM : 0.000013 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:26] Energy consumed for all GPUs : 0.000020 kWh. Total GPU Power : 30.663338242621922 W\n",
            "[codecarbon INFO @ 10:32:26] Energy consumed for all CPUs : 0.000029 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:26] 0.000063 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  12%|█▏        | 12/100 [00:02<00:18,  4.80it/s][codecarbon INFO @ 10:32:26] Energy consumed for RAM : 0.000014 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:26] Energy consumed for all GPUs : 0.000022 kWh. Total GPU Power : 28.1124518278478 W\n",
            "[codecarbon INFO @ 10:32:26] Energy consumed for all CPUs : 0.000032 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:26] 0.000068 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  13%|█▎        | 13/100 [00:02<00:18,  4.73it/s][codecarbon INFO @ 10:32:26] Energy consumed for RAM : 0.000015 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:26] Energy consumed for all GPUs : 0.000023 kWh. Total GPU Power : 30.426837907541987 W\n",
            "[codecarbon INFO @ 10:32:26] Energy consumed for all CPUs : 0.000034 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:26] 0.000073 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  14%|█▍        | 14/100 [00:02<00:17,  4.79it/s][codecarbon INFO @ 10:32:27] Energy consumed for RAM : 0.000016 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:27] Energy consumed for all GPUs : 0.000025 kWh. Total GPU Power : 28.7420086725647 W\n",
            "[codecarbon INFO @ 10:32:27] Energy consumed for all CPUs : 0.000037 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:27] 0.000078 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  15%|█▌        | 15/100 [00:03<00:17,  4.76it/s][codecarbon INFO @ 10:32:27] Energy consumed for RAM : 0.000017 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:27] Energy consumed for all GPUs : 0.000027 kWh. Total GPU Power : 29.196910226586354 W\n",
            "[codecarbon INFO @ 10:32:27] Energy consumed for all CPUs : 0.000039 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:27] 0.000083 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  16%|█▌        | 16/100 [00:03<00:17,  4.75it/s][codecarbon INFO @ 10:32:27] Energy consumed for RAM : 0.000018 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:27] Energy consumed for all GPUs : 0.000028 kWh. Total GPU Power : 29.497430641207682 W\n",
            "[codecarbon INFO @ 10:32:27] Energy consumed for all CPUs : 0.000042 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:27] 0.000089 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  17%|█▋        | 17/100 [00:03<00:17,  4.76it/s][codecarbon INFO @ 10:32:27] Energy consumed for RAM : 0.000020 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:27] Energy consumed for all GPUs : 0.000031 kWh. Total GPU Power : 40.355499612968465 W\n",
            "[codecarbon INFO @ 10:32:27] Energy consumed for all CPUs : 0.000044 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:27] 0.000095 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  18%|█▊        | 18/100 [00:03<00:17,  4.64it/s][codecarbon INFO @ 10:32:28] Energy consumed for RAM : 0.000021 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:28] Energy consumed for all GPUs : 0.000033 kWh. Total GPU Power : 30.15317650587016 W\n",
            "[codecarbon INFO @ 10:32:28] Energy consumed for all CPUs : 0.000047 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:28] 0.000100 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  19%|█▉        | 19/100 [00:04<00:17,  4.72it/s][codecarbon INFO @ 10:32:28] Energy consumed for RAM : 0.000022 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:28] Energy consumed for all GPUs : 0.000034 kWh. Total GPU Power : 28.247639943303657 W\n",
            "[codecarbon INFO @ 10:32:28] Energy consumed for all CPUs : 0.000049 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:28] 0.000105 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  20%|██        | 20/100 [00:04<00:17,  4.68it/s][codecarbon INFO @ 10:32:28] Energy consumed for RAM : 0.000023 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:28] Energy consumed for all GPUs : 0.000036 kWh. Total GPU Power : 29.106398164224128 W\n",
            "[codecarbon INFO @ 10:32:28] Energy consumed for all CPUs : 0.000052 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:28] 0.000111 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  21%|██        | 21/100 [00:04<00:16,  4.70it/s][codecarbon INFO @ 10:32:28] Energy consumed for RAM : 0.000024 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:28] Energy consumed for all GPUs : 0.000038 kWh. Total GPU Power : 30.01988761641353 W\n",
            "[codecarbon INFO @ 10:32:28] Energy consumed for all CPUs : 0.000054 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:28] 0.000116 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  22%|██▏       | 22/100 [00:04<00:16,  4.75it/s][codecarbon INFO @ 10:32:28] Energy consumed for RAM : 0.000025 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:28] Energy consumed for all GPUs : 0.000039 kWh. Total GPU Power : 26.920615547283894 W\n",
            "[codecarbon INFO @ 10:32:28] Energy consumed for all CPUs : 0.000057 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:28] 0.000121 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  23%|██▎       | 23/100 [00:04<00:16,  4.63it/s][codecarbon INFO @ 10:32:29] Energy consumed for RAM : 0.000026 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:29] Energy consumed for all GPUs : 0.000041 kWh. Total GPU Power : 29.939303005304108 W\n",
            "[codecarbon INFO @ 10:32:29] Energy consumed for all CPUs : 0.000059 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:29] 0.000126 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  24%|██▍       | 24/100 [00:05<00:16,  4.70it/s][codecarbon INFO @ 10:32:29] Energy consumed for RAM : 0.000027 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:29] Energy consumed for all GPUs : 0.000043 kWh. Total GPU Power : 28.71323661458897 W\n",
            "[codecarbon INFO @ 10:32:29] Energy consumed for all CPUs : 0.000062 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:29] 0.000132 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  25%|██▌       | 25/100 [00:05<00:15,  4.69it/s][codecarbon INFO @ 10:32:29] Energy consumed for RAM : 0.000028 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:29] Energy consumed for all GPUs : 0.000044 kWh. Total GPU Power : 29.910956659940876 W\n",
            "[codecarbon INFO @ 10:32:29] Energy consumed for all CPUs : 0.000064 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:29] 0.000137 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  26%|██▌       | 26/100 [00:05<00:15,  4.75it/s][codecarbon INFO @ 10:32:29] Energy consumed for RAM : 0.000029 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:29] Energy consumed for all GPUs : 0.000046 kWh. Total GPU Power : 30.281543820109796 W\n",
            "[codecarbon INFO @ 10:32:29] Energy consumed for all CPUs : 0.000067 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:29] 0.000142 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  27%|██▋       | 27/100 [00:05<00:15,  4.80it/s][codecarbon INFO @ 10:32:29] Energy consumed for RAM : 0.000030 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:29] Energy consumed for all GPUs : 0.000048 kWh. Total GPU Power : 43.92796687236459 W\n",
            "[codecarbon INFO @ 10:32:29] Energy consumed for all CPUs : 0.000069 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:29] 0.000148 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  28%|██▊       | 28/100 [00:05<00:15,  4.79it/s][codecarbon INFO @ 10:32:30] Energy consumed for RAM : 0.000031 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:30] Energy consumed for all GPUs : 0.000050 kWh. Total GPU Power : 30.77992092246862 W\n",
            "[codecarbon INFO @ 10:32:30] Energy consumed for all CPUs : 0.000071 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:30] 0.000153 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  29%|██▉       | 29/100 [00:06<00:14,  4.85it/s][codecarbon INFO @ 10:32:30] Energy consumed for RAM : 0.000033 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:30] Energy consumed for all GPUs : 0.000052 kWh. Total GPU Power : 29.090841626636642 W\n",
            "[codecarbon INFO @ 10:32:30] Energy consumed for all CPUs : 0.000074 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:30] 0.000158 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  30%|███       | 30/100 [00:06<00:14,  4.82it/s][codecarbon INFO @ 10:32:30] Energy consumed for RAM : 0.000034 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:30] Energy consumed for all GPUs : 0.000053 kWh. Total GPU Power : 28.699381260887794 W\n",
            "[codecarbon INFO @ 10:32:30] Energy consumed for all CPUs : 0.000076 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:30] 0.000163 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  31%|███       | 31/100 [00:06<00:14,  4.76it/s][codecarbon INFO @ 10:32:30] Energy consumed for RAM : 0.000035 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:30] Energy consumed for all GPUs : 0.000055 kWh. Total GPU Power : 28.785672465255676 W\n",
            "[codecarbon INFO @ 10:32:30] Energy consumed for all CPUs : 0.000079 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:30] 0.000169 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  32%|███▏      | 32/100 [00:06<00:14,  4.74it/s][codecarbon INFO @ 10:32:31] Energy consumed for RAM : 0.000036 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:31] Energy consumed for all GPUs : 0.000057 kWh. Total GPU Power : 24.54342452565482 W\n",
            "[codecarbon INFO @ 10:32:31] Energy consumed for all CPUs : 0.000082 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:31] 0.000174 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  33%|███▎      | 33/100 [00:06<00:14,  4.49it/s][codecarbon INFO @ 10:32:31] Energy consumed for RAM : 0.000037 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:31] Energy consumed for all GPUs : 0.000059 kWh. Total GPU Power : 34.95214024766353 W\n",
            "[codecarbon INFO @ 10:32:31] Energy consumed for all CPUs : 0.000085 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:31] 0.000181 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  34%|███▍      | 34/100 [00:07<00:15,  4.26it/s][codecarbon INFO @ 10:32:31] Energy consumed for RAM : 0.000039 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:31] Energy consumed for all GPUs : 0.000062 kWh. Total GPU Power : 30.273765925321296 W\n",
            "[codecarbon INFO @ 10:32:31] Energy consumed for all CPUs : 0.000088 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:31] 0.000189 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  35%|███▌      | 35/100 [00:07<00:16,  3.92it/s][codecarbon WARNING @ 10:32:31] Background scheduler didn't run for a long period (0s), results might be inaccurate\n",
            "[codecarbon INFO @ 10:32:31] Energy consumed for RAM : 0.000041 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:31] Energy consumed for all GPUs : 0.000064 kWh. Total GPU Power : 29.3499067709321 W\n",
            "[codecarbon INFO @ 10:32:31] Energy consumed for all CPUs : 0.000092 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:31] 0.000197 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  36%|███▌      | 36/100 [00:07<00:17,  3.67it/s][codecarbon INFO @ 10:32:32] Energy consumed for RAM : 0.000042 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:32] Energy consumed for all GPUs : 0.000067 kWh. Total GPU Power : 31.74091415108401 W\n",
            "[codecarbon INFO @ 10:32:32] Energy consumed for all CPUs : 0.000095 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:32] 0.000204 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  37%|███▋      | 37/100 [00:08<00:17,  3.60it/s][codecarbon WARNING @ 10:32:32] Background scheduler didn't run for a long period (0s), results might be inaccurate\n",
            "[codecarbon INFO @ 10:32:32] Energy consumed for RAM : 0.000044 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:32] Energy consumed for all GPUs : 0.000069 kWh. Total GPU Power : 29.45380292040888 W\n",
            "[codecarbon INFO @ 10:32:32] Energy consumed for all CPUs : 0.000099 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:32] 0.000212 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  38%|███▊      | 38/100 [00:08<00:17,  3.47it/s][codecarbon WARNING @ 10:32:32] Background scheduler didn't run for a long period (0s), results might be inaccurate\n",
            "[codecarbon INFO @ 10:32:32] Energy consumed for RAM : 0.000045 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:32] Energy consumed for all GPUs : 0.000072 kWh. Total GPU Power : 29.195520560489538 W\n",
            "[codecarbon INFO @ 10:32:32] Energy consumed for all CPUs : 0.000103 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:32] 0.000220 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  39%|███▉      | 39/100 [00:08<00:18,  3.38it/s][codecarbon INFO @ 10:32:33] Energy consumed for RAM : 0.000047 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:33] Energy consumed for all GPUs : 0.000074 kWh. Total GPU Power : 30.96679377067185 W\n",
            "[codecarbon INFO @ 10:32:33] Energy consumed for all CPUs : 0.000106 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:33] 0.000227 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  40%|████      | 40/100 [00:09<00:17,  3.38it/s][codecarbon INFO @ 10:32:33] Energy consumed for RAM : 0.000048 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:33] Energy consumed for all GPUs : 0.000077 kWh. Total GPU Power : 33.118515620471236 W\n",
            "[codecarbon INFO @ 10:32:33] Energy consumed for all CPUs : 0.000109 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:33] 0.000234 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  41%|████      | 41/100 [00:09<00:17,  3.44it/s][codecarbon INFO @ 10:32:33] Energy consumed for RAM : 0.000050 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:33] Energy consumed for all GPUs : 0.000078 kWh. Total GPU Power : 25.043145660042036 W\n",
            "[codecarbon INFO @ 10:32:33] Energy consumed for all CPUs : 0.000112 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:33] 0.000240 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  42%|████▏     | 42/100 [00:09<00:16,  3.61it/s][codecarbon INFO @ 10:32:33] Energy consumed for RAM : 0.000051 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:33] Energy consumed for all GPUs : 0.000081 kWh. Total GPU Power : 43.823302130100664 W\n",
            "[codecarbon INFO @ 10:32:33] Energy consumed for all CPUs : 0.000115 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:33] 0.000246 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  43%|████▎     | 43/100 [00:09<00:14,  3.89it/s][codecarbon INFO @ 10:32:34] Energy consumed for RAM : 0.000052 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:34] Energy consumed for all GPUs : 0.000083 kWh. Total GPU Power : 28.197221730719406 W\n",
            "[codecarbon INFO @ 10:32:34] Energy consumed for all CPUs : 0.000117 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:34] 0.000251 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  44%|████▍     | 44/100 [00:10<00:13,  4.08it/s][codecarbon INFO @ 10:32:34] Energy consumed for RAM : 0.000053 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:34] Energy consumed for all GPUs : 0.000084 kWh. Total GPU Power : 30.48000844676728 W\n",
            "[codecarbon INFO @ 10:32:34] Energy consumed for all CPUs : 0.000120 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:34] 0.000257 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  45%|████▌     | 45/100 [00:10<00:12,  4.31it/s][codecarbon INFO @ 10:32:34] Energy consumed for RAM : 0.000054 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:34] Energy consumed for all GPUs : 0.000086 kWh. Total GPU Power : 28.824790013820223 W\n",
            "[codecarbon INFO @ 10:32:34] Energy consumed for all CPUs : 0.000122 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:34] 0.000262 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  46%|████▌     | 46/100 [00:10<00:12,  4.41it/s][codecarbon INFO @ 10:32:34] Energy consumed for RAM : 0.000055 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:34] Energy consumed for all GPUs : 0.000087 kWh. Total GPU Power : 28.112149256255442 W\n",
            "[codecarbon INFO @ 10:32:34] Energy consumed for all CPUs : 0.000125 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:34] 0.000267 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  47%|████▋     | 47/100 [00:10<00:11,  4.46it/s][codecarbon INFO @ 10:32:34] Energy consumed for RAM : 0.000056 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:34] Energy consumed for all GPUs : 0.000089 kWh. Total GPU Power : 29.60849047988026 W\n",
            "[codecarbon INFO @ 10:32:34] Energy consumed for all CPUs : 0.000127 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:34] 0.000272 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  48%|████▊     | 48/100 [00:10<00:11,  4.55it/s][codecarbon INFO @ 10:32:35] Energy consumed for RAM : 0.000057 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:35] Energy consumed for all GPUs : 0.000091 kWh. Total GPU Power : 28.99045789960707 W\n",
            "[codecarbon INFO @ 10:32:35] Energy consumed for all CPUs : 0.000130 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:35] 0.000278 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  49%|████▉     | 49/100 [00:11<00:11,  4.60it/s][codecarbon INFO @ 10:32:35] Energy consumed for RAM : 0.000058 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:35] Energy consumed for all GPUs : 0.000092 kWh. Total GPU Power : 29.340874533540216 W\n",
            "[codecarbon INFO @ 10:32:35] Energy consumed for all CPUs : 0.000132 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:35] 0.000283 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  50%|█████     | 50/100 [00:11<00:10,  4.65it/s][codecarbon INFO @ 10:32:35] Energy consumed for RAM : 0.000059 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:35] Energy consumed for all GPUs : 0.000094 kWh. Total GPU Power : 29.945831481449158 W\n",
            "[codecarbon INFO @ 10:32:35] Energy consumed for all CPUs : 0.000134 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:35] 0.000288 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  51%|█████     | 51/100 [00:11<00:10,  4.71it/s][codecarbon INFO @ 10:32:35] Energy consumed for RAM : 0.000060 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:35] Energy consumed for all GPUs : 0.000096 kWh. Total GPU Power : 29.14468104193809 W\n",
            "[codecarbon INFO @ 10:32:35] Energy consumed for all CPUs : 0.000137 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:35] 0.000293 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  52%|█████▏    | 52/100 [00:11<00:10,  4.72it/s][codecarbon INFO @ 10:32:35] Energy consumed for RAM : 0.000062 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:35] Energy consumed for all GPUs : 0.000097 kWh. Total GPU Power : 27.97337025795858 W\n",
            "[codecarbon INFO @ 10:32:35] Energy consumed for all CPUs : 0.000139 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:35] 0.000298 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  53%|█████▎    | 53/100 [00:11<00:10,  4.66it/s][codecarbon INFO @ 10:32:36] Energy consumed for RAM : 0.000063 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:36] Energy consumed for all GPUs : 0.000099 kWh. Total GPU Power : 28.841952926863936 W\n",
            "[codecarbon INFO @ 10:32:36] Energy consumed for all CPUs : 0.000142 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:36] 0.000304 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  54%|█████▍    | 54/100 [00:12<00:09,  4.67it/s][codecarbon INFO @ 10:32:36] Energy consumed for RAM : 0.000064 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:36] Energy consumed for all GPUs : 0.000101 kWh. Total GPU Power : 30.14996084752119 W\n",
            "[codecarbon INFO @ 10:32:36] Energy consumed for all CPUs : 0.000144 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:36] 0.000309 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  55%|█████▌    | 55/100 [00:12<00:09,  4.73it/s][codecarbon INFO @ 10:32:36] Energy consumed for RAM : 0.000065 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:36] Energy consumed for all GPUs : 0.000102 kWh. Total GPU Power : 30.63919624882517 W\n",
            "[codecarbon INFO @ 10:32:36] Energy consumed for all CPUs : 0.000147 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:36] 0.000314 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  56%|█████▌    | 56/100 [00:12<00:09,  4.80it/s][codecarbon INFO @ 10:32:36] Energy consumed for RAM : 0.000066 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:36] Energy consumed for all GPUs : 0.000104 kWh. Total GPU Power : 30.58634893599083 W\n",
            "[codecarbon INFO @ 10:32:36] Energy consumed for all CPUs : 0.000149 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:36] 0.000319 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  57%|█████▋    | 57/100 [00:12<00:08,  4.86it/s][codecarbon INFO @ 10:32:36] Energy consumed for RAM : 0.000067 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:36] Energy consumed for all GPUs : 0.000106 kWh. Total GPU Power : 28.125831172583986 W\n",
            "[codecarbon INFO @ 10:32:36] Energy consumed for all CPUs : 0.000152 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:36] 0.000324 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  58%|█████▊    | 58/100 [00:12<00:08,  4.76it/s][codecarbon INFO @ 10:32:37] Energy consumed for RAM : 0.000068 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:37] Energy consumed for all GPUs : 0.000107 kWh. Total GPU Power : 28.881725489023363 W\n",
            "[codecarbon INFO @ 10:32:37] Energy consumed for all CPUs : 0.000154 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:37] 0.000329 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  59%|█████▉    | 59/100 [00:13<00:08,  4.74it/s][codecarbon INFO @ 10:32:37] Energy consumed for RAM : 0.000069 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:37] Energy consumed for all GPUs : 0.000109 kWh. Total GPU Power : 27.974229307177584 W\n",
            "[codecarbon INFO @ 10:32:37] Energy consumed for all CPUs : 0.000157 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:37] 0.000335 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  60%|██████    | 60/100 [00:13<00:08,  4.69it/s][codecarbon INFO @ 10:32:37] Energy consumed for RAM : 0.000070 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:37] Energy consumed for all GPUs : 0.000111 kWh. Total GPU Power : 30.227744679841994 W\n",
            "[codecarbon INFO @ 10:32:37] Energy consumed for all CPUs : 0.000159 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:37] 0.000340 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  61%|██████    | 61/100 [00:13<00:08,  4.74it/s][codecarbon INFO @ 10:32:37] Energy consumed for RAM : 0.000071 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:37] Energy consumed for all GPUs : 0.000113 kWh. Total GPU Power : 42.56653474147079 W\n",
            "[codecarbon INFO @ 10:32:37] Energy consumed for all CPUs : 0.000162 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:37] 0.000346 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  62%|██████▏   | 62/100 [00:13<00:08,  4.71it/s][codecarbon INFO @ 10:32:38] Energy consumed for RAM : 0.000072 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:38] Energy consumed for all GPUs : 0.000115 kWh. Total GPU Power : 27.63539173686292 W\n",
            "[codecarbon INFO @ 10:32:38] Energy consumed for all CPUs : 0.000164 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:38] 0.000351 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  63%|██████▎   | 63/100 [00:14<00:07,  4.65it/s][codecarbon INFO @ 10:32:38] Energy consumed for RAM : 0.000073 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:38] Energy consumed for all GPUs : 0.000117 kWh. Total GPU Power : 30.136156256662684 W\n",
            "[codecarbon INFO @ 10:32:38] Energy consumed for all CPUs : 0.000167 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:38] 0.000357 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  64%|██████▍   | 64/100 [00:14<00:07,  4.70it/s][codecarbon INFO @ 10:32:38] Energy consumed for RAM : 0.000075 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:38] Energy consumed for all GPUs : 0.000118 kWh. Total GPU Power : 29.340454399836656 W\n",
            "[codecarbon INFO @ 10:32:38] Energy consumed for all CPUs : 0.000169 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:38] 0.000362 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  65%|██████▌   | 65/100 [00:14<00:07,  4.72it/s][codecarbon INFO @ 10:32:38] Energy consumed for RAM : 0.000076 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:38] Energy consumed for all GPUs : 0.000120 kWh. Total GPU Power : 29.614615254605397 W\n",
            "[codecarbon INFO @ 10:32:38] Energy consumed for all CPUs : 0.000171 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:38] 0.000367 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  66%|██████▌   | 66/100 [00:14<00:07,  4.74it/s][codecarbon INFO @ 10:32:38] Energy consumed for RAM : 0.000077 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:38] Energy consumed for all GPUs : 0.000122 kWh. Total GPU Power : 28.682060413038556 W\n",
            "[codecarbon INFO @ 10:32:38] Energy consumed for all CPUs : 0.000174 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:38] 0.000372 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  67%|██████▋   | 67/100 [00:14<00:06,  4.72it/s][codecarbon INFO @ 10:32:39] Energy consumed for RAM : 0.000078 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:39] Energy consumed for all GPUs : 0.000123 kWh. Total GPU Power : 29.21194682084269 W\n",
            "[codecarbon INFO @ 10:32:39] Energy consumed for all CPUs : 0.000176 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:39] 0.000377 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  68%|██████▊   | 68/100 [00:15<00:06,  4.72it/s][codecarbon INFO @ 10:32:39] Energy consumed for RAM : 0.000079 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:39] Energy consumed for all GPUs : 0.000125 kWh. Total GPU Power : 28.135459612226864 W\n",
            "[codecarbon INFO @ 10:32:39] Energy consumed for all CPUs : 0.000179 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:39] 0.000383 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  69%|██████▉   | 69/100 [00:15<00:06,  4.67it/s][codecarbon INFO @ 10:32:39] Energy consumed for RAM : 0.000080 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:39] Energy consumed for all GPUs : 0.000127 kWh. Total GPU Power : 30.492677145557213 W\n",
            "[codecarbon INFO @ 10:32:39] Energy consumed for all CPUs : 0.000181 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:39] 0.000388 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  70%|███████   | 70/100 [00:15<00:06,  4.75it/s][codecarbon INFO @ 10:32:39] Energy consumed for RAM : 0.000081 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:39] Energy consumed for all GPUs : 0.000129 kWh. Total GPU Power : 42.84482706466163 W\n",
            "[codecarbon INFO @ 10:32:39] Energy consumed for all CPUs : 0.000184 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:39] 0.000394 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  71%|███████   | 71/100 [00:15<00:06,  4.72it/s][codecarbon INFO @ 10:32:39] Energy consumed for RAM : 0.000082 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:39] Energy consumed for all GPUs : 0.000131 kWh. Total GPU Power : 29.956452961382634 W\n",
            "[codecarbon INFO @ 10:32:39] Energy consumed for all CPUs : 0.000186 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:39] 0.000399 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  72%|███████▏  | 72/100 [00:15<00:05,  4.76it/s][codecarbon INFO @ 10:32:40] Energy consumed for RAM : 0.000083 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:40] Energy consumed for all GPUs : 0.000133 kWh. Total GPU Power : 27.539835634617745 W\n",
            "[codecarbon INFO @ 10:32:40] Energy consumed for all CPUs : 0.000189 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:40] 0.000405 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  73%|███████▎  | 73/100 [00:16<00:05,  4.67it/s][codecarbon INFO @ 10:32:40] Energy consumed for RAM : 0.000084 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:40] Energy consumed for all GPUs : 0.000134 kWh. Total GPU Power : 28.363387747479077 W\n",
            "[codecarbon INFO @ 10:32:40] Energy consumed for all CPUs : 0.000191 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:40] 0.000410 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  74%|███████▍  | 74/100 [00:16<00:05,  4.65it/s][codecarbon INFO @ 10:32:40] Energy consumed for RAM : 0.000086 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:40] Energy consumed for all GPUs : 0.000136 kWh. Total GPU Power : 28.727215420040928 W\n",
            "[codecarbon INFO @ 10:32:40] Energy consumed for all CPUs : 0.000194 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:40] 0.000415 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  75%|███████▌  | 75/100 [00:16<00:05,  4.66it/s][codecarbon INFO @ 10:32:40] Energy consumed for RAM : 0.000087 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:40] Energy consumed for all GPUs : 0.000138 kWh. Total GPU Power : 30.131496755083724 W\n",
            "[codecarbon INFO @ 10:32:40] Energy consumed for all CPUs : 0.000196 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:40] 0.000420 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  76%|███████▌  | 76/100 [00:16<00:05,  4.71it/s][codecarbon INFO @ 10:32:41] Energy consumed for RAM : 0.000088 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:41] Energy consumed for all GPUs : 0.000139 kWh. Total GPU Power : 30.485156688355385 W\n",
            "[codecarbon INFO @ 10:32:41] Energy consumed for all CPUs : 0.000199 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:41] 0.000425 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  77%|███████▋  | 77/100 [00:17<00:04,  4.77it/s][codecarbon INFO @ 10:32:41] Energy consumed for RAM : 0.000089 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:41] Energy consumed for all GPUs : 0.000141 kWh. Total GPU Power : 26.92731307813832 W\n",
            "[codecarbon INFO @ 10:32:41] Energy consumed for all CPUs : 0.000201 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:41] 0.000431 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  78%|███████▊  | 78/100 [00:17<00:04,  4.65it/s][codecarbon INFO @ 10:32:41] Energy consumed for RAM : 0.000090 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:41] Energy consumed for all GPUs : 0.000143 kWh. Total GPU Power : 28.133511595550107 W\n",
            "[codecarbon INFO @ 10:32:41] Energy consumed for all CPUs : 0.000204 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:41] 0.000436 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  79%|███████▉  | 79/100 [00:17<00:04,  4.63it/s][codecarbon INFO @ 10:32:41] Energy consumed for RAM : 0.000091 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:41] Energy consumed for all GPUs : 0.000144 kWh. Total GPU Power : 29.812127914170674 W\n",
            "[codecarbon INFO @ 10:32:41] Energy consumed for all CPUs : 0.000206 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:41] 0.000441 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  80%|████████  | 80/100 [00:17<00:04,  4.69it/s][codecarbon INFO @ 10:32:41] Energy consumed for RAM : 0.000092 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:41] Energy consumed for all GPUs : 0.000146 kWh. Total GPU Power : 30.3660855153845 W\n",
            "[codecarbon INFO @ 10:32:41] Energy consumed for all CPUs : 0.000209 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:41] 0.000446 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  81%|████████  | 81/100 [00:17<00:03,  4.75it/s][codecarbon INFO @ 10:32:42] Energy consumed for RAM : 0.000093 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:42] Energy consumed for all GPUs : 0.000148 kWh. Total GPU Power : 28.23779531490039 W\n",
            "[codecarbon INFO @ 10:32:42] Energy consumed for all CPUs : 0.000211 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:42] 0.000452 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  82%|████████▏ | 82/100 [00:18<00:03,  4.70it/s][codecarbon INFO @ 10:32:42] Energy consumed for RAM : 0.000094 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:42] Energy consumed for all GPUs : 0.000149 kWh. Total GPU Power : 29.57622112212526 W\n",
            "[codecarbon INFO @ 10:32:42] Energy consumed for all CPUs : 0.000214 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:42] 0.000457 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  83%|████████▎ | 83/100 [00:18<00:03,  4.73it/s][codecarbon INFO @ 10:32:42] Energy consumed for RAM : 0.000095 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:42] Energy consumed for all GPUs : 0.000151 kWh. Total GPU Power : 29.1210510584119 W\n",
            "[codecarbon INFO @ 10:32:42] Energy consumed for all CPUs : 0.000216 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:42] 0.000462 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  84%|████████▍ | 84/100 [00:18<00:03,  4.72it/s][codecarbon INFO @ 10:32:42] Energy consumed for RAM : 0.000096 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:42] Energy consumed for all GPUs : 0.000153 kWh. Total GPU Power : 28.86825409417382 W\n",
            "[codecarbon INFO @ 10:32:42] Energy consumed for all CPUs : 0.000219 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:42] 0.000468 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  85%|████████▌ | 85/100 [00:18<00:03,  4.71it/s][codecarbon INFO @ 10:32:42] Energy consumed for RAM : 0.000097 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:42] Energy consumed for all GPUs : 0.000154 kWh. Total GPU Power : 29.35417518896877 W\n",
            "[codecarbon INFO @ 10:32:42] Energy consumed for all CPUs : 0.000221 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:42] 0.000473 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  86%|████████▌ | 86/100 [00:18<00:02,  4.72it/s][codecarbon INFO @ 10:32:43] Energy consumed for RAM : 0.000099 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:43] Energy consumed for all GPUs : 0.000156 kWh. Total GPU Power : 29.344835414984992 W\n",
            "[codecarbon INFO @ 10:32:43] Energy consumed for all CPUs : 0.000223 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:43] 0.000478 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  87%|████████▋ | 87/100 [00:19<00:02,  4.73it/s][codecarbon INFO @ 10:32:43] Energy consumed for RAM : 0.000100 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:43] Energy consumed for all GPUs : 0.000158 kWh. Total GPU Power : 29.421731857102316 W\n",
            "[codecarbon INFO @ 10:32:43] Energy consumed for all CPUs : 0.000226 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:43] 0.000483 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  88%|████████▊ | 88/100 [00:19<00:02,  4.74it/s][codecarbon INFO @ 10:32:43] Energy consumed for RAM : 0.000101 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:43] Energy consumed for all GPUs : 0.000159 kWh. Total GPU Power : 24.343246280049858 W\n",
            "[codecarbon INFO @ 10:32:43] Energy consumed for all CPUs : 0.000229 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:43] 0.000489 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  89%|████████▉ | 89/100 [00:19<00:02,  4.48it/s][codecarbon WARNING @ 10:32:43] Background scheduler didn't run for a long period (0s), results might be inaccurate\n",
            "[codecarbon INFO @ 10:32:43] Energy consumed for RAM : 0.000103 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:43] Energy consumed for all GPUs : 0.000162 kWh. Total GPU Power : 28.40688020472413 W\n",
            "[codecarbon INFO @ 10:32:43] Energy consumed for all CPUs : 0.000233 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:43] 0.000497 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  90%|█████████ | 90/100 [00:19<00:02,  3.95it/s][codecarbon WARNING @ 10:32:44] Background scheduler didn't run for a long period (0s), results might be inaccurate\n",
            "[codecarbon INFO @ 10:32:44] Energy consumed for RAM : 0.000104 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:44] Energy consumed for all GPUs : 0.000164 kWh. Total GPU Power : 29.05985134123799 W\n",
            "[codecarbon INFO @ 10:32:44] Energy consumed for all CPUs : 0.000236 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:44] 0.000505 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  91%|█████████ | 91/100 [00:20<00:02,  3.67it/s][codecarbon WARNING @ 10:32:44] Background scheduler didn't run for a long period (0s), results might be inaccurate\n",
            "[codecarbon INFO @ 10:32:44] Energy consumed for RAM : 0.000106 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:44] Energy consumed for all GPUs : 0.000168 kWh. Total GPU Power : 37.79682584314455 W\n",
            "[codecarbon INFO @ 10:32:44] Energy consumed for all CPUs : 0.000240 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:44] 0.000514 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  92%|█████████▏| 92/100 [00:20<00:02,  3.48it/s][codecarbon WARNING @ 10:32:44] Background scheduler didn't run for a long period (0s), results might be inaccurate\n",
            "[codecarbon INFO @ 10:32:44] Energy consumed for RAM : 0.000108 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:44] Energy consumed for all GPUs : 0.000170 kWh. Total GPU Power : 28.892370555604444 W\n",
            "[codecarbon INFO @ 10:32:44] Energy consumed for all CPUs : 0.000244 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:44] 0.000522 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  93%|█████████▎| 93/100 [00:20<00:02,  3.37it/s][codecarbon WARNING @ 10:32:45] Background scheduler didn't run for a long period (0s), results might be inaccurate\n",
            "[codecarbon INFO @ 10:32:45] Energy consumed for RAM : 0.000109 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:45] Energy consumed for all GPUs : 0.000173 kWh. Total GPU Power : 28.962157560138937 W\n",
            "[codecarbon INFO @ 10:32:45] Energy consumed for all CPUs : 0.000248 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:45] 0.000529 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  94%|█████████▍| 94/100 [00:21<00:01,  3.29it/s][codecarbon WARNING @ 10:32:45] Background scheduler didn't run for a long period (0s), results might be inaccurate\n",
            "[codecarbon INFO @ 10:32:45] Energy consumed for RAM : 0.000111 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:45] Energy consumed for all GPUs : 0.000175 kWh. Total GPU Power : 29.35821703193806 W\n",
            "[codecarbon INFO @ 10:32:45] Energy consumed for all CPUs : 0.000251 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:45] 0.000537 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  95%|█████████▌| 95/100 [00:21<00:01,  3.26it/s][codecarbon INFO @ 10:32:45] Energy consumed for RAM : 0.000112 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:45] Energy consumed for all GPUs : 0.000177 kWh. Total GPU Power : 27.709233947508 W\n",
            "[codecarbon INFO @ 10:32:45] Energy consumed for all CPUs : 0.000254 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:45] 0.000543 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  96%|█████████▌| 96/100 [00:21<00:01,  3.55it/s][codecarbon INFO @ 10:32:45] Energy consumed for RAM : 0.000113 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:45] Energy consumed for all GPUs : 0.000178 kWh. Total GPU Power : 30.315112791151503 W\n",
            "[codecarbon INFO @ 10:32:45] Energy consumed for all CPUs : 0.000256 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:45] 0.000548 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  97%|█████████▋| 97/100 [00:21<00:00,  3.87it/s][codecarbon INFO @ 10:32:46] Energy consumed for RAM : 0.000114 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:46] Energy consumed for all GPUs : 0.000180 kWh. Total GPU Power : 29.43680341988547 W\n",
            "[codecarbon INFO @ 10:32:46] Energy consumed for all CPUs : 0.000259 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:46] 0.000553 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  98%|█████████▊| 98/100 [00:22<00:00,  4.10it/s][codecarbon INFO @ 10:32:46] Energy consumed for RAM : 0.000115 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:46] Energy consumed for all GPUs : 0.000182 kWh. Total GPU Power : 29.352459729075104 W\n",
            "[codecarbon INFO @ 10:32:46] Energy consumed for all CPUs : 0.000261 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:46] 0.000558 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling:  99%|█████████▉| 99/100 [00:22<00:00,  4.29it/s][codecarbon INFO @ 10:32:46] Energy consumed for RAM : 0.000116 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:46] Energy consumed for all GPUs : 0.000183 kWh. Total GPU Power : 29.269965593119334 W\n",
            "[codecarbon INFO @ 10:32:46] Energy consumed for all CPUs : 0.000264 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:46] 0.000563 kWh of electricity used since the beginning.\n",
            "Energy/CO₂ Emissions Profiling: 100%|██████████| 100/100 [00:22<00:00,  4.43it/s]\n",
            "[codecarbon INFO @ 10:32:46] Energy consumed for RAM : 0.000116 kWh. RAM Power : 19.122615337371826 W\n",
            "[codecarbon INFO @ 10:32:46] Energy consumed for all GPUs : 0.000183 kWh. Total GPU Power : 0.0 W\n",
            "[codecarbon INFO @ 10:32:46] Energy consumed for all CPUs : 0.000264 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 10:32:46] 0.000563 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"energy_consumption\": {\n",
            "        \"iterations\": 100,\n",
            "        \"input_duration_sec\": 10.0,\n",
            "        \"avg_emission_rate_gCO2eq_per_sec\": 0.0007429383310169242,\n",
            "        \"avg_cpu_energy_kWh\": 0.00026368899262613725,\n",
            "        \"avg_ram_energy_kWh\": 0.00011633221994473461\n",
            "    }\n",
            "}\n",
            "Saved (updated) energy profiling results to: ./3_profiling_results_COLAB/model_info_20250409_1028.json\n",
            "Saved compressed NPZ energy data to: ./3_profiling_results_COLAB/energy_co2_20250409_1028.npz\n",
            "\n",
            "\n",
            "EOF\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/codecarbon/output_methods/file.py:43: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n",
            "/usr/local/lib/python3.11/dist-packages/codecarbon/output_methods/file.py:72: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architectural Profiling"
      ],
      "metadata": {
        "id": "QkaLRqBNSYlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from epanns_inference import models\n",
        "\n",
        "# Ensure reproducibility\n",
        "set_seeds(42)\n",
        "\n",
        "# Output Parameters -----------------------------------------------------------------------------\n",
        "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "RESULTS_DIR = \"./3_profiling_results_COLAB/E2PANNs_architecture/\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "HMykEASKPHZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Parameters\n",
        "SAMPLE_RATE = 32000\n",
        "INPUT_DURATION_SEC = 10.0\n",
        "CHECKPOINT_PATH = \"./multi-unified.ckpt\"\n",
        "\n",
        "# Load the model\n",
        "model = models.Cnn14_pruned(pre_trained=False)\n",
        "model, _ = load_lightning2pt(CHECKPOINT_PATH, model, device=\"cpu\", verbose=True, validate_updates=False)\n",
        "print('\\n')\n",
        "model.cpu()\n",
        "\n",
        "# Inference Activites Trace (CPU-only Trace)\n",
        "model_trace_path = os.path.join(RESULTS_DIR, f\"inference_trace_CPU_{TIMESTAMP}.json\")\n",
        "inference_trace(model=model,\n",
        "                sample_rate=SAMPLE_RATE,\n",
        "                input_duration_sec=INPUT_DURATION_SEC,\n",
        "                device='cpu',\n",
        "                save_path=model_trace_path)\n",
        "print('\\n')\n",
        "\n",
        "# Architecture structure\n",
        "structure_log_path = model_trace_path = os.path.join(RESULTS_DIR,\"architecture_summary.log\")\n",
        "architecture_profile(model=model,\n",
        "                     sample_rate=SAMPLE_RATE,\n",
        "                     input_duration_sec=INPUT_DURATION_SEC,\n",
        "                     device='cpu',\n",
        "                     save_path=structure_log_path)\n",
        "print('\\n')\n",
        "\n",
        "# Computational Graph\n",
        "model_graph = draw_graph(model=model,\n",
        "                         input_size=(1, 320000),\n",
        "                         depth=100,\n",
        "                         graph_dir='TB',\n",
        "                         roll=True,\n",
        "                         expand_nested=True,\n",
        "                         hide_inner_tensors=False,\n",
        "                         hide_module_functions=False,\n",
        "                         device='cpu',\n",
        "                         save_graph=True,\n",
        "                         filename=f'E2PANNs_graph',\n",
        "                         directory=RESULTS_DIR)\n",
        "model_graph.visual_graph\n",
        "\n",
        "# Inference Activites Trace (GPU-accelerated)\n",
        "model.cuda()\n",
        "model_trace_path = os.path.join(RESULTS_DIR, f\"inference_trace_CUDA_{TIMESTAMP}.json\")\n",
        "inference_trace(model=model,\n",
        "                sample_rate=SAMPLE_RATE,\n",
        "                input_duration_sec=INPUT_DURATION_SEC,\n",
        "                device='cuda:0',\n",
        "                save_path=model_trace_path)\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "U5ly1uv_R47w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d53d964-3983-49b8-9343-104f66facae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected prefix 'model.'. Stripped from state_dict keys.\n",
            "Model moved to device: cpu\n",
            "State dict successfully loaded into the model!\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/epanns_inference/models/models.py:241: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  clipwise_output = nn.functional.softmax(self.fc_audioset(x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference trace successfully saved to: ./3_profiling_results_COLAB/inference_trace_CPU_20250409_1028.json\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-04-09 10:37:10.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36marchitecture_profile\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1m=================================================================================================================================================================================================================================\n",
            "Layer (type (var_name):depth-idx)                  Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds                 Trainable\n",
            "=================================================================================================================================================================================================================================\n",
            "Cnn14_pruned (Cnn14_pruned)                        [1, 320000]               [1, 2048]                 --                             --                   --                        --                        Partial\n",
            "+ Spectrogram (spectrogram_extractor): 1-1         [1, 320000]               [1, 1, 1001, 513]         --                             --                   --                        --                        False\n",
            "|    + STFT (stft): 2-1                            --                        --                        --                             --                   --                        --                        False\n",
            "|    |    + Conv1d (conv_real): 3-1                [1, 1, 321024]            [1, 513, 1001]            (525,312)                   2.16%                   [1024]                    525,837,312               False\n",
            "|    |    + Conv1d (conv_imag): 3-2                [1, 1, 321024]            [1, 513, 1001]            (525,312)                   2.16%                   [1024]                    525,837,312               False\n",
            "+ LogmelFilterBank (logmel_extractor): 1-2         [1, 1, 1001, 513]         [1, 1, 1001, 64]          (32,832)                    0.14%                   --                        --                        False\n",
            "+ BatchNorm2d (bn0): 1-3                           [1, 64, 1001, 1]          [1, 64, 1001, 1]          128                         0.00%                   --                        128                       True\n",
            "+ ConvBlock_pruned (conv_block1): 1-4              [1, 1, 1001, 64]          [1, 64, 500, 32]          --                             --                   --                        --                        True\n",
            "|    + Conv2d (conv1): 2-2                         [1, 1, 1001, 64]          [1, 64, 1001, 64]         576                         0.00%                   [3, 3]                    36,900,864                True\n",
            "|    + BatchNorm2d (bn1): 2-3                      [1, 64, 1001, 64]         [1, 64, 1001, 64]         128                         0.00%                   --                        128                       True\n",
            "|    + Conv2d (conv2): 2-4                         [1, 64, 1001, 64]         [1, 64, 1001, 64]         36,864                      0.15%                   [3, 3]                    2,361,655,296             True\n",
            "|    + BatchNorm2d (bn2): 2-5                      [1, 64, 1001, 64]         [1, 64, 1001, 64]         128                         0.00%                   --                        128                       True\n",
            "+ ConvBlock_pruned (conv_block2): 1-5              [1, 64, 500, 32]          [1, 128, 250, 16]         --                             --                   --                        --                        True\n",
            "|    + Conv2d (conv1): 2-6                         [1, 64, 500, 32]          [1, 128, 500, 32]         73,728                      0.30%                   [3, 3]                    1,179,648,000             True\n",
            "|    + BatchNorm2d (bn1): 2-7                      [1, 128, 500, 32]         [1, 128, 500, 32]         256                         0.00%                   --                        256                       True\n",
            "|    + Conv2d (conv2): 2-8                         [1, 128, 500, 32]         [1, 128, 500, 32]         147,456                     0.61%                   [3, 3]                    2,359,296,000             True\n",
            "|    + BatchNorm2d (bn2): 2-9                      [1, 128, 500, 32]         [1, 128, 500, 32]         256                         0.00%                   --                        256                       True\n",
            "+ ConvBlock_pruned (conv_block3): 1-6              [1, 128, 250, 16]         [1, 256, 125, 8]          --                             --                   --                        --                        True\n",
            "|    + Conv2d (conv1): 2-10                        [1, 128, 250, 16]         [1, 256, 250, 16]         294,912                     1.21%                   [3, 3]                    1,179,648,000             True\n",
            "|    + BatchNorm2d (bn1): 2-11                     [1, 256, 250, 16]         [1, 256, 250, 16]         512                         0.00%                   --                        512                       True\n",
            "|    + Conv2d (conv2): 2-12                        [1, 256, 250, 16]         [1, 256, 250, 16]         589,824                     2.43%                   [3, 3]                    2,359,296,000             True\n",
            "|    + BatchNorm2d (bn2): 2-13                     [1, 256, 250, 16]         [1, 256, 250, 16]         512                         0.00%                   --                        512                       True\n",
            "+ ConvBlock_pruned (conv_block4): 1-7              [1, 256, 125, 8]          [1, 256, 62, 4]           --                             --                   --                        --                        True\n",
            "|    + Conv2d (conv1): 2-14                        [1, 256, 125, 8]          [1, 256, 125, 8]          589,824                     2.43%                   [3, 3]                    589,824,000               True\n",
            "|    + BatchNorm2d (bn1): 2-15                     [1, 256, 125, 8]          [1, 256, 125, 8]          512                         0.00%                   --                        512                       True\n",
            "|    + Conv2d (conv2): 2-16                        [1, 256, 125, 8]          [1, 256, 125, 8]          589,824                     2.43%                   [3, 3]                    589,824,000               True\n",
            "|    + BatchNorm2d (bn2): 2-17                     [1, 256, 125, 8]          [1, 256, 125, 8]          512                         0.00%                   --                        512                       True\n",
            "+ ConvBlock_pruned (conv_block5): 1-8              [1, 256, 62, 4]           [1, 512, 31, 2]           --                             --                   --                        --                        True\n",
            "|    + Conv2d (conv1): 2-18                        [1, 256, 62, 4]           [1, 512, 62, 4]           1,179,648                   4.86%                   [3, 3]                    292,552,704               True\n",
            "|    + BatchNorm2d (bn1): 2-19                     [1, 512, 62, 4]           [1, 512, 62, 4]           1,024                       0.00%                   --                        1,024                     True\n",
            "|    + Conv2d (conv2): 2-20                        [1, 512, 62, 4]           [1, 512, 62, 4]           2,359,296                   9.71%                   [3, 3]                    585,105,408               True\n",
            "|    + BatchNorm2d (bn2): 2-21                     [1, 512, 62, 4]           [1, 512, 62, 4]           1,024                       0.00%                   --                        1,024                     True\n",
            "+ ConvBlock_pruned (conv_block6): 1-9              [1, 512, 31, 2]           [1, 1024, 31, 2]          --                             --                   --                        --                        True\n",
            "|    + Conv2d (conv1): 2-22                        [1, 512, 31, 2]           [1, 1024, 31, 2]          4,718,592                  19.43%                   [3, 3]                    292,552,704               True\n",
            "|    + BatchNorm2d (bn1): 2-23                     [1, 1024, 31, 2]          [1, 1024, 31, 2]          2,048                       0.01%                   --                        2,048                     True\n",
            "|    + Conv2d (conv2): 2-24                        [1, 1024, 31, 2]          [1, 1024, 31, 2]          9,437,184                  38.85%                   [3, 3]                    585,105,408               True\n",
            "|    + BatchNorm2d (bn2): 2-25                     [1, 1024, 31, 2]          [1, 1024, 31, 2]          2,048                       0.01%                   --                        2,048                     True\n",
            "+ Linear (fc1): 1-10                               [1, 1024]                 [1, 2048]                 2,099,200                   8.64%                   --                        2,099,200                 True\n",
            "+ Linear (fc_audioset): 1-11                       [1, 2048]                 [1, 527]                  1,079,823                   4.45%                   --                        1,079,823                 True\n",
            "=================================================================================================================================================================================================================================\n",
            "Total params: 24,289,295\n",
            "Trainable params: 23,205,839\n",
            "Non-trainable params: 1,083,456\n",
            "Total mult-adds (Units.GIGABYTES): 13.47\n",
            "=================================================================================================================================================================================================================================\n",
            "Input size (MB): 1.28\n",
            "Forward/backward pass size (MB): 253.06\n",
            "Params size (MB): 97.16\n",
            "Estimated Total Size (MB): 351.49\n",
            "=================================================================================================================================================================================================================================\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
            "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
            "/usr/local/lib/python3.11/dist-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::pad\". Skipped.\n",
            "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
            "/usr/local/lib/python3.11/dist-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::pow\". Skipped.\n",
            "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
            "/usr/local/lib/python3.11/dist-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::log10\". Skipped.\n",
            "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
            "/usr/local/lib/python3.11/dist-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::sub_\". Skipped.\n",
            "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
            "/usr/local/lib/python3.11/dist-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::max\". Skipped.\n",
            "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
            "\u001b[32m2025-04-09 10:37:10.661\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36marchitecture_profile\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mMACs    : 13516473024\u001b[0m\n",
            "\u001b[32m2025-04-09 10:37:10.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36marchitecture_profile\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mFLOPs   : 27032946048\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_tensor.py:1648: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  ret = func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference trace successfully saved to: ./3_profiling_results_COLAB/inference_trace_CUDA_20250409_1028.json\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/epanns_inference/models/models.py:241: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  clipwise_output = nn.functional.softmax(self.fc_audioset(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coefficients Visualization"
      ],
      "metadata": {
        "id": "uChQ-aNOcNbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove old RunTimes Directory\n",
        "!rm -rf weights_plots"
      ],
      "metadata": {
        "id": "X2qh3hvDnhFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Weights Visualization with Plotly - Save to HTML Files ----------------\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "\n",
        "def weights_html_plots(model, device=\"cpu\", save_dir=\"./weights_plots\", kernel_separation=5):\n",
        "    \"\"\"\n",
        "    Visualize model weights using Plotly and save each figure as a standalone HTML file.\n",
        "    - Conv2d layers: separated 3D kernels vertically with gray colormap and simplified colorbar (only min and max).\n",
        "    - Conv1d and Linear layers: interactive lollipop plots.\n",
        "    Each layer's visualization is in its own HTML file.\n",
        "\n",
        "    :param model: PyTorch model instance.\n",
        "    :param device: Device to move the model on ('cpu' or 'cuda').\n",
        "    :param save_dir: Directory where to save the HTML files.\n",
        "    :param kernel_separation: Vertical separation between Conv2D kernels (default: 5 units).\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Helper: Recursively yield named modules\n",
        "    def named_modules_recursive(model, prefix=\"\"):\n",
        "        for name, module in model.named_children():\n",
        "            full_name = f\"{prefix}.{name}\" if prefix else name\n",
        "            yield full_name, module\n",
        "            yield from named_modules_recursive(module, full_name)\n",
        "\n",
        "    for name, layer in named_modules_recursive(model):\n",
        "        try:\n",
        "            safe_layer_name = name.replace(\".\", \"_\")  # Safe filename\n",
        "\n",
        "            if isinstance(layer, torch.nn.Conv2d):\n",
        "                weights = layer.weight.data.detach().cpu().numpy()  # (out_channels, in_channels, H, W)\n",
        "                out_channels, in_channels, height, width = weights.shape\n",
        "\n",
        "                cube = weights[:, 0, :, :]  # Take first input channel only\n",
        "\n",
        "                fig = go.Figure()\n",
        "\n",
        "                min_val = np.min(cube)\n",
        "                max_val = np.max(cube)\n",
        "\n",
        "                for i in range(cube.shape[0]):\n",
        "                    fig.add_trace(go.Surface(z=np.full_like(cube[i], i * kernel_separation),  # Apply vertical offset\n",
        "                                  surfacecolor=cube[i],\n",
        "                                  colorscale='gray',\n",
        "                                  showscale=(i == 0),  # Only show colorbar once\n",
        "                                  opacity=0.8,\n",
        "                                  colorbar=dict(title=\"Weight Value\",\n",
        "                                    titleside=\"right\",\n",
        "                                    tickmode=\"array\",\n",
        "                                    tickvals=[min_val, max_val],\n",
        "                                    ticktext=[f\"{min_val:.2f}\", f\"{max_val:.2f}\"],\n",
        "                                    lenmode=\"pixels\",\n",
        "                                    len=200)))\n",
        "\n",
        "                fig.update_layout(title=f\"Conv2D Layer: {name}\",\n",
        "                                  scene=dict(xaxis_title='Width',\n",
        "                                             yaxis_title='Height',\n",
        "                                             zaxis_title='Kernel Index',\n",
        "                                             aspectmode='data'),\n",
        "                                  autosize=True,\n",
        "                                  height=1000,\n",
        "                                  width=1000,\n",
        "                                  template=\"simple_white\")\n",
        "\n",
        "                html_path = os.path.join(save_dir, f\"{safe_layer_name}.html\")\n",
        "                pio.write_html(fig, file=html_path, auto_open=False)\n",
        "                print(f\"Saved Conv2D figure: {html_path}\")\n",
        "\n",
        "            elif isinstance(layer, (torch.nn.Conv1d, torch.nn.Linear)):\n",
        "                weights = layer.weight.data.detach().cpu().flatten()\n",
        "\n",
        "                fig = go.Figure()\n",
        "\n",
        "                fig.add_trace(go.Scatter(x=np.arange(len(weights)),\n",
        "                                         y=weights,\n",
        "                                         mode=\"markers+lines\",\n",
        "                                         marker=dict(size=6, color='black'),\n",
        "                                         line=dict(color='gray'),\n",
        "                                         hoverinfo='x+y'))\n",
        "\n",
        "                fig.update_layout(title=f\"Layer: {name}\",\n",
        "                                  xaxis_title=\"Index\",\n",
        "                                  yaxis_title=\"Weight Value\",\n",
        "                                  height=1000,\n",
        "                                  width=1000,\n",
        "                                  template=\"simple_white\")\n",
        "\n",
        "                html_path = os.path.join(save_dir, f\"{safe_layer_name}.html\")\n",
        "                pio.write_html(fig, file=html_path, auto_open=False)\n",
        "                print(f\"Saved Linear/Conv1D figure: {html_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[WARNING] Skipping layer {name} due to error: {e}\")"
      ],
      "metadata": {
        "id": "iM-Ydc1GOKFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RESULTS_DIR = \"./3_profiling_results_COLAB\"\n",
        "weights_html_plots(model, device=\"cpu\", save_dir=f\"./{RESULTS_DIR}/weights_plots\", kernel_separation=2)"
      ],
      "metadata": {
        "id": "hIafQMEJcX81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "610e102f-af68-407a-9f73-368d4bb50210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Linear/Conv1D figure: ././3_profiling_results_COLAB/weights_plots/spectrogram_extractor_stft_conv_real.html\n",
            "Saved Linear/Conv1D figure: ././3_profiling_results_COLAB/weights_plots/spectrogram_extractor_stft_conv_imag.html\n",
            "Saved Conv2D figure: ././3_profiling_results_COLAB/weights_plots/conv_block1_conv1.html\n",
            "Saved Conv2D figure: ././3_profiling_results_COLAB/weights_plots/conv_block1_conv2.html\n",
            "Saved Conv2D figure: ././3_profiling_results_COLAB/weights_plots/conv_block2_conv1.html\n",
            "Saved Conv2D figure: ././3_profiling_results_COLAB/weights_plots/conv_block2_conv2.html\n",
            "Saved Conv2D figure: ././3_profiling_results_COLAB/weights_plots/conv_block3_conv1.html\n",
            "Saved Conv2D figure: ././3_profiling_results_COLAB/weights_plots/conv_block3_conv2.html\n",
            "Saved Conv2D figure: ././3_profiling_results_COLAB/weights_plots/conv_block4_conv1.html\n",
            "Saved Conv2D figure: ././3_profiling_results_COLAB/weights_plots/conv_block4_conv2.html\n",
            "Saved Conv2D figure: ././3_profiling_results_COLAB/weights_plots/conv_block5_conv1.html\n",
            "Saved Conv2D figure: ././3_profiling_results_COLAB/weights_plots/conv_block5_conv2.html\n",
            "Saved Conv2D figure: ././3_profiling_results_COLAB/weights_plots/conv_block6_conv1.html\n",
            "Saved Conv2D figure: ././3_profiling_results_COLAB/weights_plots/conv_block6_conv2.html\n",
            "Saved Linear/Conv1D figure: ././3_profiling_results_COLAB/weights_plots/fc1.html\n",
            "Saved Linear/Conv1D figure: ././3_profiling_results_COLAB/weights_plots/fc_audioset.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a zipped version of assets\n",
        "!zip -r 3_profiling_results_COLAB.zip 3_profiling_results_COLAB\n",
        "from google.colab import files\n",
        "files.download('3_profiling_results_COLAB.zip')"
      ],
      "metadata": {
        "id": "s1slDtTaDSYm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "22164582-76d6-47ce-b36f-a886fd812327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: 3_profiling_results_COLAB/ (stored 0%)\n",
            "  adding: 3_profiling_results_COLAB/gpu_info_20250409_1028.json (deflated 52%)\n",
            "  adding: 3_profiling_results_COLAB/hardware_info_20250409_1028.json (deflated 80%)\n",
            "  adding: 3_profiling_results_COLAB/model_info_20250409_1028.json (deflated 72%)\n",
            "  adding: 3_profiling_results_COLAB/cpu_overall_times_20250409_1028.npz (deflated 4%)\n",
            "  adding: 3_profiling_results_COLAB/cpu_process_times_20250409_1028.npz (deflated 4%)\n",
            "  adding: 3_profiling_results_COLAB/memory_and_cache_20250409_1028.npz (deflated 25%)\n",
            "  adding: 3_profiling_results_COLAB/cpu_usage_20250409_1028.npz (deflated 11%)\n",
            "  adding: 3_profiling_results_COLAB/cuda_times_20250409_1028.npz (deflated 2%)\n",
            "  adding: 3_profiling_results_COLAB/e2e_inference_times_20250409_1028.npz (deflated 2%)\n",
            "  adding: 3_profiling_results_COLAB/energy_emissions.csv (deflated 38%)\n",
            "  adding: 3_profiling_results_COLAB/emissions_base_8bf2183a-5368-4d3f-9f3d-06ca7300ce6b.csv (deflated 80%)\n",
            "  adding: 3_profiling_results_COLAB/energy_co2_20250409_1028.npz (deflated 58%)\n",
            "  adding: 3_profiling_results_COLAB/inference_trace_CPU_20250409_1028.json (deflated 93%)\n",
            "  adding: 3_profiling_results_COLAB/architecture_summary.log (deflated 87%)\n",
            "  adding: 3_profiling_results_COLAB/E2PANNs_graph (deflated 96%)\n",
            "  adding: 3_profiling_results_COLAB/E2PANNs_graph.png (deflated 10%)\n",
            "  adding: 3_profiling_results_COLAB/inference_trace_CUDA_20250409_1028.json (deflated 93%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/ (stored 0%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/spectrogram_extractor_stft_conv_real.html (deflated 65%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/spectrogram_extractor_stft_conv_imag.html (deflated 65%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/conv_block1_conv1.html (deflated 71%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/conv_block1_conv2.html (deflated 71%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/conv_block2_conv1.html (deflated 71%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/conv_block2_conv2.html (deflated 71%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/conv_block3_conv1.html (deflated 72%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/conv_block3_conv2.html (deflated 72%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/conv_block4_conv1.html (deflated 72%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/conv_block4_conv2.html (deflated 72%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/conv_block5_conv1.html (deflated 72%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/conv_block5_conv2.html (deflated 72%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/conv_block6_conv1.html (deflated 74%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/conv_block6_conv2.html (deflated 74%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/fc1.html (deflated 68%)\n",
            "  adding: 3_profiling_results_COLAB/weights_plots/fc_audioset.html (deflated 67%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_041fcd7a-93c8-46a2-aaec-6df00d8294b2\", \"3_profiling_results_COLAB.zip\", 51214469)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fu83wvSTIzzB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}